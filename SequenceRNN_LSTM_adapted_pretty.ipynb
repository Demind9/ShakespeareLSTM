{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM CharRNN - with different encoding strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "deIiS_8W67vH"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils as utils\n",
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_ag_S2ct0ueI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 100\n",
      "{'L': 0, '!': 1, ';': 2, '?': 3, 'W': 4, ':': 5, 'R': 6, 'v': 7, 'p': 8, 'i': 9, 'a': 10, 't': 11, 'f': 12, 'M': 13, 'E': 14, 'g': 15, '-': 16, 'y': 17, 'G': 18, '&': 19, 'o': 20, 's': 21, 'd': 22, 'u': 23, \"'\": 24, 'H': 25, 'U': 26, 'r': 27, 'x': 28, 'I': 29, 'D': 30, 'q': 31, 'b': 32, 'Q': 33, 'c': 34, '.': 35, 'l': 36, 'm': 37, 'z': 38, 'O': 39, 'j': 40, ' ': 41, 'V': 42, 'F': 43, 'B': 44, 'Y': 45, 'T': 46, ',': 47, 'n': 48, '\\n': 49, '3': 50, 'C': 51, 'w': 52, 'e': 53, 'X': 54, 'P': 55, 'S': 56, '$': 57, 'A': 58, 'K': 59, 'Z': 60, 'N': 61, 'k': 62, 'h': 63, 'J': 64}\n",
      "1403 4225\n",
      "{'Fi': 0, 'ir': 1, 'rs': 2, 'st': 3, 't ': 4, ' C': 5, 'Ci': 6, 'it': 7, 'ti': 8, 'iz': 9, 'ze': 10, 'en': 11, 'n:': 12, ':\\n': 13, '\\nB': 14, 'Be': 15, 'ef': 16, 'fo': 17, 'or': 18, 're': 19, 'e ': 20, ' w': 21, 'we': 22, ' p': 23, 'pr': 24, 'ro': 25, 'oc': 26, 'ce': 27, 'ee': 28, 'ed': 29, 'd ': 30, ' a': 31, 'an': 32, 'ny': 33, 'y ': 34, ' f': 35, 'fu': 36, 'ur': 37, 'rt': 38, 'th': 39, 'he': 40, 'er': 41, 'r,': 42, ', ': 43, ' h': 44, 'ea': 45, 'ar': 46, 'r ': 47, ' m': 48, 'me': 49, ' s': 50, 'sp': 51, 'pe': 52, 'ak': 53, 'k.': 54, '.\\n': 55, '\\n\\n': 56, '\\nA': 57, 'Al': 58, 'll': 59, 'l:': 60, '\\nS': 61, 'Sp': 62, 'k,': 63, '\\nF': 64, '\\nY': 65, 'Yo': 66, 'ou': 67, 'u ': 68, 'al': 69, 'l ': 70, ' r': 71, 'es': 72, 'so': 73, 'ol': 74, 'lv': 75, 've': 76, 'ra': 77, 'at': 78, ' t': 79, 'to': 80, 'o ': 81, ' d': 82, 'di': 83, 'ie': 84, 'ha': 85, 'n ': 86, 'fa': 87, 'am': 88, 'mi': 89, 'is': 90, 'sh': 91, 'h?': 92, '?\\n': 93, '\\nR': 94, 'Re': 95, 'd.': 96, '. ': 97, 't,': 98, ' y': 99, 'yo': 100, ' k': 101, 'kn': 102, 'no': 103, 'ow': 104, 'w ': 105, 'Ca': 106, 'ai': 107, 'iu': 108, 'us': 109, 's ': 110, ' M': 111, 'Ma': 112, 'rc': 113, 'ci': 114, ' i': 115, ' c': 116, 'ch': 117, 'hi': 118, 'f ': 119, ' e': 120, 'ne': 121, 'em': 122, 'my': 123, 'eo': 124, 'op': 125, 'pl': 126, 'le': 127, 'e.': 128, '\\nW': 129, 'We': 130, \"w'\": 131, \"'t\": 132, 't.': 133, '\\nL': 134, 'Le': 135, 'et': 136, ' u': 137, 'ki': 138, 'il': 139, 'im': 140, 'm,': 141, 'nd': 142, \"e'\": 143, \"'l\": 144, 'av': 145, 'co': 146, 'rn': 147, ' o': 148, 'wn': 149, 'ri': 150, 'ic': 151, '\\nI': 152, 'Is': 153, \"s'\": 154, 'a ': 155, ' v': 156, 'rd': 157, 'ct': 158, 't?': 159, '\\nN': 160, 'No': 161, 'mo': 162, 'ta': 163, 'lk': 164, 'in': 165, 'ng': 166, 'g ': 167, 'on': 168, \"n'\": 169, 't;': 170, '; ': 171, ' l': 172, ' b': 173, 'be': 174, 'do': 175, 'e:': 176, ': ': 177, 'aw': 178, 'wa': 179, 'ay': 180, 'y,': 181, 'y!': 182, '!\\n': 183, 'Se': 184, 'ec': 185, '\\nO': 186, 'On': 187, 'wo': 188, 'd,': 189, ' g': 190, 'go': 191, 'oo': 192, 'od': 193, 'ns': 194, 's.': 195, 'ac': 196, 'cc': 197, 'un': 198, 'nt': 199, 'te': 200, 'po': 201, 's,': 202, 'pa': 203, 'tr': 204, 'ia': 205, 'Wh': 206, 'au': 207, 'ut': 208, 'ho': 209, 'ty': 210, 'su': 211, 'rf': 212, 'fe': 213, 'ei': 214, 'ts': 215, 'ul': 216, 'ld': 217, 'el': 218, 'li': 219, 'ev': 220, 's:': 221, 'if': 222, 'ey': 223, 'y\\n': 224, '\\nw': 225, 'yi': 226, 'bu': 227, 'up': 228, 'fl': 229, 'lu': 230, 'ui': 231, 'wh': 232, 'e\\n': 233, 'om': 234, 'e,': 235, 'ig': 236, 'gh': 237, 'ht': 238, 'gu': 239, 'ue': 240, 'ss': 241, 'hu': 242, 'um': 243, 'ma': 244, 'ly': 245, 'y;': 246, ';\\n': 247, '\\nb': 248, 'nk': 249, 'k ': 250, 'de': 251, 'r:': 252, 'nn': 253, 't\\n': 254, '\\na': 255, 'af': 256, 'ff': 257, 'ob': 258, 'bj': 259, 'je': 260, 'of': 261, 'se': 262, 'ry': 263, 'as': 264, 'n\\n': 265, '\\ni': 266, 'nv': 267, 'cu': 268, 'la': 269, 'ab': 270, 'da': 271, 'nc': 272, 'e;': 273, 'r\\n': 274, '\\ns': 275, 'uf': 276, 'ga': 277, 'm ': 278, ' L': 279, 'ge': 280, 'wi': 281, 'h\\n': 282, '\\no': 283, 'pi': 284, 'ik': 285, 'ke': 286, 'ds': 287, ' I': 288, 'I\\n': 289, 'br': 290, 'ad': 291, ' n': 292, 'ot': 293, 'Wo': 294, 'ag': 295, 's?': 296, 'Ag': 297, 'fi': 298, 't:': 299, \"'s\": 300, 'og': 301, 'mm': 302, 'na': 303, 'lt': 304, 'y.': 305, '\\nC': 306, 'Co': 307, 'si': 308, 'id': 309, 'rv': 310, 'vi': 311, 'y?': 312, '\\nV': 313, 'Ve': 314, 'l;': 315, 'gi': 316, 'iv': 317, 'd\\n': 318, '\\nr': 319, 'ep': 320, 'ys': 321, 'ms': 322, 'lf': 323, 'h ': 324, 'ud': 325, 'Na': 326, 'io': 327, 'sl': 328, 'I ': 329, 'sa': 330, 'u,': 331, 'd:': 332, 'ug': 333, 'ft': 334, 't-': 335, '-c': 336, 'sc': 337, 'ca': 338, '\\nc': 339, 'o\\n': 340, '\\np': 341, 'tl': 342, 'd;': 343, 'tu': 344, 'lp': 345, 'p ': 346, 'a\\n': 347, '\\nv': 348, 'm.': 349, ' Y': 350, 'mu': 351, 'ov': 352, 'If': 353, 'ba': 354, 'rr': 355, 's;': 356, '\\nh': 357, 'rp': 358, 'n.': 359, 'e?': 360, '? ': 361, ' T': 362, 'Th': 363, \"o'\": 364, \"' \": 365, 'hy': 366, 'ap': 367, 'l!': 368, 'So': 369, 't!': 370, '! ': 371, 'Me': 372, 'ni': 373, ' A': 374, 'gr': 375, 'ip': 376, 'pp': 377, 'a;': 378, 'lw': 379, 'lo': 380, '\\nt': 381, '\\nH': 382, 'He': 383, 'h:': 384, 'o!': 385, '\\nM': 386, 'ME': 387, 'EN': 388, 'NE': 389, 'NI': 390, 'IU': 391, 'US': 392, 'S:': 393, 'rk': 394, \"k'\": 395, 'ym': 396, 'n,': 397, 'd?': 398, 'u\\n': 399, 'Wi': 400, 'cl': 401, 'ub': 402, 'bs': 403, 'tt': 404, 'r?': 405, 'u.': 406, 'Ou': 407, 'kl': 408, 'tn': 409, 'o,': 410, ',\\n': 411, \" '\": 412, \"'e\": 413, 'hs': 414, 'rm': 415, 'o.': 416, 'fr': 417, 'hb': 418, 'bo': 419, 'lr': 420, 'dy': 421, 'os': 422, 'bl': 423, 'Ha': 424, ' F': 425, 'Fo': 426, 'h,': 427, 'l\\n': 428, 'St': 429, 'm\\n': 430, ' R': 431, 'Ro': 432, '\\nT': 433, 'cr': 434, 'ck': 435, 'rb': 436, 's\\n': 437, 'Of': 438, 'Ap': 439, 'mp': 440, 'p.': 441, 'by': 442, 'lm': 443, 's!': 444, 'Tr': 445, 'ru': 446, 'd!': 447, '\\ny': 448, 'ye': 449, 'e-': 450, '-h': 451, 'n;': 452, '\\ne': 453, 'r.': 454, 'p,': 455, '\\nE': 456, 'Ei': 457, 'nf': 458, 'dr': 459, 'Or': 460, 'A ': 461, 'Bu': 462, 'pu': 463, 'To': 464, 'l,': 465, \"I'\": 466, '\\nf': 467, 'b ': 468, 'sg': 469, \"y'\": 470, 'mb': 471, 'eb': 472, \"l'\": 473, \"'d\": 474, 'nl': 475, 'dl': 476, 'pb': 477, 'oa': 478, 'g\\n': 479, 'Li': 480, '\\nD': 481, 'Di': 482, 'uc': 483, 'An': 484, 'ua': 485, '\\nU': 486, 'Un': 487, 'sw': 488, \"r'\": 489, 'd-': 490, '--': 491, '-\\n': 492, 'Si': 493, ' W': 494, 'sm': 495, 'gs': 496, 's-': 497, 'ok': 498, 'As': 499, 'k-': 500, '-i': 501, 'gl': 502, 'pt': 503, 'gn': 504, 'y-': 505, 'eg': 506, 'g,': 507, 'ps': 508, 'In': 509, 'c,': 510, 'n?': 511, \"\\n'\": 512, \"'F\": 513, 'ks': 514, 'Sh': 515, ',-': 516, \"u'\": 517, 'l-': 518, '-o': 519, '\\nP': 520, 'Pa': 521, 'Ye': 522, \"'r\": 523, 'ib': 524, \"'T\": 525, \",'\": 526, ' q': 527, 'qu': 528, 'uo': 529, 'p\\n': 530, 'y:': 531, 'hr': 532, 'Ev': 533, 'Fr': 534, 'cy': 535, \"'-\": 536, '-t': 537, 'Ay': 538, 'r;': 539, 'l.': 540, \".'\": 541, 'It': 542, 'ex': 543, 'xa': 544, 'ls': 545, 'c ': 546, 'oe': 547, 'e!': 548, \"d'\": 549, 'va': 550, 'oi': 551, 'MA': 552, 'AR': 553, 'RC': 554, 'CI': 555, \"t'\": 556, 'bb': 557, 'bi': 558, 'tc': 559, 'bh': 560, 'g.': 561, ' H': 562, 'ox': 563, 'xe': 564, 'bd': 565, 'du': 566, ' j': 567, 'ju': 568, 'De': 569, 'Up': 570, 'vo': 571, 'ew': 572, 'ws': 573, 'nu': 574, 'Hi': 575, 'rl': 576, 'ek': 577, 'g?': 578, 'f,': 579, 'm!': 580, 'w\\n': 581, \"i'\": 582, 'nj': 583, '\\ng': 584, 'h!': 585, 'hl': 586, ' B': 587, 'p?': 588, 'n-': 589, \"h'\": 590, 'm?': 591, 'vu': 592, 'lg': 593, 'sd': 594, ' J': 595, 'Ju': 596, 'Br': 597, ' V': 598, \"-'\": 599, \"'S\": 600, 'Sd': 601, 'nr': 602, \"f'\": 603, 'Er': 604, 'rg': 605, '\\nG': 606, 'Go': 607, 'gm': 608, 'Vo': 609, \"a'\": 610, ' S': 611, 'Tu': 612, 'Au': 613, 'vy': 614, 'CO': 615, 'OM': 616, 'MI': 617, 'IN': 618, 'm:': 619, 'At': 620, 'Ti': 621, 'La': 622, 'f?': 623, 'TI': 624, 'IT': 625, 'TU': 626, \"'o\": 627, 'eh': 628, 'O,': 629, '-b': 630, 'w,': 631, 'w:': 632, 'pf': 633, 'w.': 634, 'SI': 635, 'IC': 636, 'Wa': 637, 'BR': 638, 'RU': 639, 'UT': 640, 'eq': 641, '-m': 642, 'Su': 643, 'Fa': 644, 'tm': 645, 'dd': 646, \"'O\": 647, 'O ': 648, \"!'\": 649, \"'\\n\": 650, 'Op': 651, 'Ho': 652, 'Mo': 653, 'AU': 654, 'UF': 655, 'FI': 656, 'ID': 657, 'DI': 658, 'mv': 659, 'k\\n': 660, 'u:': 661, 'bt': 662, \"m'\": 663, 'By': 664, 'Ta': 665, 'i:': 666, \"'v\": 667, ' N': 668, 'rw': 669, 'tw': 670, 'u!': 671, 'VO': 672, 'OL': 673, 'LU': 674, 'UM': 675, 'MN': 676, 'IA': 677, 'A:': 678, 'g;': 679, 'xp': 680, '\\nm': 681, 'mf': 682, 'sb': 683, 'ej': 684, 'jo': 685, 'r-': 686, 'b,': 687, 'az': 688, 'I,': 689, '\\nn': 690, '-l': 691, 'f\\n': 692, '\\nd': 693, 'm;': 694, 'oy': 695, 'VI': 696, 'IR': 697, 'RG': 698, 'GI': 699, 'IL': 700, 'LI': 701, 'oz': 702, 'Ge': 703, 'Va': 704, 'f.': 705, \"'C\": 706, \":'\": 707, 'sk': 708, 'w!': 709, ' O': 710, 'Aw': 711, 'ph': 712, 'a,': 713, ' G': 714, 'Gr': 715, 'mn': 716, 'Te': 717, 'lc': 718, \"'h\": 719, 'VA': 720, 'AL': 721, 'LE': 722, 'ER': 723, 'RI': 724, 'My': 725, 'Sw': 726, '-k': 727, 'h.': 728, 'p;': 729, '\\nl': 730, \"O'\": 731, 'dn': 732, 'gt': 733, 'u?': 734, ' P': 735, 'Pe': 736, 'ya': 737, ' U': 738, 'Ul': 739, 'c\\n': 740, 'xc': 741, 'u;': 742, 'h;': 743, 'i;': 744, 'Gi': 745, 'Pr': 746, 'Vi': 747, 'LA': 748, 'RT': 749, 'Sa': 750, 'w;': 751, 'uy': 752, 'k!': 753, 'Ar': 754, 'Ra': 755, 'p:': 756, 'f!': 757, 'Am': 758, 'Ad': 759, 'dv': 760, 'wr': 761, 'dg': 762, 'f-': 763, '-B': 764, 'Bo': 765, 'Pl': 766, 'Fu': 767, 'r!': 768, 'I.': 769, 'Sl': 770, 'Cl': 771, \"p'\": 772, 'td': 773, 'Lo': 774, 'hm': 775, 'Cu': 776, 'Ir': 777, 'Pi': 778, 'Mi': 779, '-p': 780, 'kf': 781, 'i ': 782, 'Ab': 783, 'dw': 784, 'Fl': 785, 'xi': 786, 'yh': 787, '-a': 788, '!-': 789, 'l?': 790, 'o?': 791, 'g:': 792, 'f;': 793, 'eu': 794, 'Fe': 795, 'Af': 796, 'ix': 797, 'x ': 798, 'Wr': 799, 'gg': 800, \"g'\": 801, 'dm': 802, \"'W\": 803, 'xt': 804, \"'g\": 805, 'Ne': 806, '-f': 807, 'g!': 808, '.-': 809, 'yp': 810, 'i,': 811, 'CA': 812, 'AI': 813, 'S ': 814, 'OR': 815, 'IO': 816, 'AN': 817, 'NU': 818, 'S!': 819, 'o:': 820, 'wb': 821, 'Mu': 822, 'k:': 823, 'i\\n': 824, ' D': 825, 'n!': 826, 'Tw': 827, 'f:': 828, 'Em': 829, 'rq': 830, 'u-': 831, 'o-': 832, '-n': 833, 'b.': 834, 'ae': 835, 'Es': 836, '-w': 837, 'nm': 838, '\\nu': 839, '-I': 840, 'Ly': 841, 'yc': 842, 'aj': 843, 'sy': 844, 'yl': 845, '\\nk': 846, '-s': 847, '-d': 848, 'a!': 849, 'Ga': 850, '\\nK': 851, 'Kn': 852, 'wl': 853, '?-': 854, '-C': 855, 'b-': 856, 'nh': 857, 'kr': 858, \"'b\": 859, 'Do': 860, '-g': 861, 'wd': 862, 'Ph': 863, 'Du': 864, 'p-': 865, 'fs': 866, 'dk': 867, 'Jo': 868, 'sn': 869, 'uk': 870, 'fy': 871, 'k?': 872, 'df': 873, 'zo': 874, 'w-': 875, '-e': 876, 'ax': 877, 'k;': 878, '-q': 879, 'Ru': 880, 'Pu': 881, 'tp': 882, 'o;': 883, 'ku': 884, \";'\": 885, 'hw': 886, \"'I\": 887, '-P': 888, ':-': 889, \"'L\": 890, 'Bi': 891, 'Ki': 892, 'a?': 893, 'iq': 894, 'x\\n': 895, 'En': 896, 'nw': 897, 'Ce': 898, \"'a\": 899, 'ii': 900, 'Ty': 901, 'Nu': 902, ' Q': 903, 'Qu': 904, '  ': 905, ' \\n': 906, 'Sc': 907, 'za': 908, 'Yi': 909, 'w?': 910, 'nq': 911, 'Ea': 912, 'Ch': 913, \"'?\": 914, \"'!\": 915, 'Hy': 916, 'yd': 917, 'nb': 918, \"'w\": 919, ' &': 920, '&C': 921, 'C:': 922, 'AE': 923, 'Ed': 924, \"'P\": 925, \"'B\": 926, 'hd': 927, 'c;': 928, 'c.': 929, 'ka': 930, 'wf': 931, 'm-': 932, 'Ac': 933, 'oq': 934, 'lb': 935, 'Sm': 936, 'p!': 937, \"'m\": 938, 'yr': 939, \"'D\": 940, 'ml': 941, 'Ke': 942, 'Dr': 943, 'bm': 944, \"'G\": 945, \"b'\": 946, '-y': 947, \"x'\": 948, 'a-': 949, 'xs': 950, 'Ba': 951, ' E': 952, 'Ni': 953, 'yf': 954, \"'f\": 955, 'rj': 956, 'h-': 957, 'py': 958, ' K': 959, 'mt': 960, '- ': 961, '\\nq': 962, 'I;': 963, 'xy': 964, 'gy': 965, 'ko': 966, 'Bl': 967, 'Cr': 968, ';-': 969, '\\nJ': 970, 'c-': 971, 'hc': 972, 'sq': 973, 'b\\n': 974, 'Ri': 975, 'cq': 976, 'ao': 977, \"'R\": 978, 'np': 979, 'ah': 980, 'Ja': 981, 'yn': 982, 'tf': 983, 'ln': 984, 'dc': 985, 'Ol': 986, 'cs': 987, \"A'\": 988, 'Da': 989, 'rh': 990, 'hn': 991, 'O!': 992, 'kb': 993, 'oj': 994, \"'n\": 995, 'i?': 996, \"'y\": 997, \"'H\": 998, \"'M\": 999, 'GL': 1000, 'LO': 1001, 'OU': 1002, 'UC': 1003, 'CE': 1004, 'ES': 1005, 'ST': 1006, 'TE': 1007, 'R:': 1008, '-v': 1009, 'g-': 1010, \"G'\": 1011, 'CL': 1012, 'RE': 1013, 'NC': 1014, 'E:': 1015, '-r': 1016, 'G.': 1017, 'G\\n': 1018, 'G,': 1019, 'Hu': 1020, 'RA': 1021, 'AK': 1022, 'KE': 1023, 'NB': 1024, 'BU': 1025, 'UR': 1026, 'RY': 1027, 'Y:': 1028, 'HA': 1029, 'AS': 1030, 'NG': 1031, 'GS': 1032, 'uz': 1033, 'zz': 1034, 'AD': 1035, 'DY': 1036, 'Y ': 1037, 'NN': 1038, 'Po': 1039, 'Av': 1040, 'xh': 1041, 'Im': 1042, 'GE': 1043, 'NT': 1044, 'TL': 1045, 'EM': 1046, 'N:': 1047, '-F': 1048, 'wk': 1049, 'IV': 1050, 'VE': 1051, 'RS': 1052, 'GR': 1053, 'EY': 1054, '\\nQ': 1055, 'QU': 1056, 'UE': 1057, 'EE': 1058, 'N ': 1059, 'EL': 1060, 'IZ': 1061, 'ZA': 1062, 'AB': 1063, 'BE': 1064, 'ET': 1065, 'TH': 1066, 'H:': 1067, 'Oh': 1068, 'Gl': 1069, 'CK': 1070, 'KI': 1071, 'GH': 1072, 'AM': 1073, 'M:': 1074, 'DE': 1075, 'RB': 1076, 'BY': 1077, 'yw': 1078, 'ih': 1079, 'Ai': 1080, \"':\": 1081, 'GA': 1082, 'T:': 1083, 'Je': 1084, 'gd': 1085, 'DO': 1086, 'SE': 1087, 'Ex': 1088, 'b!': 1089, 'Ur': 1090, 'ky': 1091, 'AT': 1092, 'SB': 1093, 'hf': 1094, 'a:': 1095, 'a.': 1096, \"?'\": 1097, \"'j\": 1098, \"'Z\": 1099, 'Zo': 1100, 'Ah': 1101, 'I:': 1102, 'G ': 1103, 'ED': 1104, 'DW': 1105, 'WA': 1106, 'RD': 1107, 'D ': 1108, 'V:': 1109, ' z': 1110, 'Ox': 1111, 'xf': 1112, 'DU': 1113, 'CH': 1114, 'HE': 1115, 'SS': 1116, 'OF': 1117, 'F ': 1118, 'YO': 1119, 'RK': 1120, 'K:': 1121, ' :': 1122, 'Lu': 1123, 'HB': 1124, 'BI': 1125, 'IS': 1126, 'SH': 1127, 'HO': 1128, 'OP': 1129, 'P ': 1130, '-S': 1131, 'wt': 1132, \"'A\": 1133, 'PR': 1134, 'E ': 1135, 'D:': 1136, 'hh': 1137, 'NA': 1138, 'L:': 1139, 'zi': 1140, 'tg': 1141, \"'c\": 1142, 'TA': 1143, 'NL': 1144, 'oh': 1145, 'TC': 1146, 'IF': 1147, 'FF': 1148, 'F:': 1149, 'UG': 1150, 'sf': 1151, 'LY': 1152, 'El': 1153, \"'i\": 1154, 'OV': 1155, 'Gu': 1156, 'dh': 1157, 'ux': 1158, 'xu': 1159, 'ez': 1160, \"'z\": 1161, 'NO': 1162, 'OT': 1163, 'II': 1164, 'TY': 1165, 'YR': 1166, 'RR': 1167, 'I?': 1168, 'iw': 1169, 'Us': 1170, 'Il': 1171, 'HR': 1172, 'TO': 1173, 'PH': 1174, 'HM': 1175, 'MO': 1176, 'ON': 1177, 'ND': 1178, 'mw': 1179, 'OX': 1180, 'XF': 1181, 'FO': 1182, 'BL': 1183, 'UN': 1184, 'SU': 1185, 'RF': 1186, 'LK': 1187, 'Gh': 1188, 'DS': 1189, 'wy': 1190, '-u': 1191, 'JO': 1192, 'OH': 1193, 'HN': 1194, 'NR': 1195, 'BO': 1196, 'GB': 1197, 'RO': 1198, 'OK': 1199, 'OW': 1200, 'WB': 1201, 'AY': 1202, 'gb': 1203, 'Ob': 1204, 'gf': 1205, 'UK': 1206, 'RL': 1207, 'HY': 1208, 'i-': 1209, 'HU': 1210, 'MB': 1211, '-G': 1212, 'OS': 1213, 'WI': 1214, 'LL': 1215, 'BA': 1216, 'AG': 1217, 'GO': 1218, 'PE': 1219, 'CY': 1220, 'wm': 1221, 'EA': 1222, 'L ': 1223, 'SA': 1224, 'SL': 1225, 'R ': 1226, 'EP': 1227, 'SC': 1228, 'CR': 1229, 'OO': 1230, 'P:': 1231, 'Sn': 1232, \"'K\": 1233, 'ja': 1234, 'TZ': 1235, 'ZW': 1236, 'tz': 1237, 'zw': 1238, 'Ov': 1239, \"'J\": 1240, \"'p\": 1241, 'i.': 1242, 'EX': 1243, 'XT': 1244, 'x,': 1245, 'MP': 1246, 'PS': 1247, 'SO': 1248, 'EG': 1249, 'AH': 1250, 'NV': 1251, 'O:': 1252, 'YB': 1253, 'LT': 1254, 'AP': 1255, 'PU': 1256, 'UL': 1257, 'GU': 1258, '-H': 1259, 'yb': 1260, 'wu': 1261, 'EO': 1262, 'z,': 1263, 'z.': 1264, 'xq': 1265, 'PA': 1266, 'kw': 1267, 'JU': 1268, 'IE': 1269, '-N': 1270, 'hq': 1271, \"'Y\": 1272, 'x.': 1273, 'CU': 1274, 'zy': 1275, 'hp': 1276, 'Et': 1277, 'x;': 1278, '-M': 1279, 'dj': 1280, 'Ec': 1281, 'yv': 1282, '-j': 1283, 'FR': 1284, 'b;': 1285, 'i!': 1286, '-L': 1287, '-O': 1288, 'R.': 1289, 'I!': 1290, '-W': 1291, \"c'\": 1292, 'c!': 1293, '-T': 1294, \"';\": 1295, 'aq': 1296, 'O?': 1297, 'Cy': 1298, 'b:': 1299, 'zl': 1300, 'Ut': 1301, 'dp': 1302, 'kt': 1303, 'xo': 1304, 'Ey': 1305, 'RW': 1306, 'WE': 1307, 'TM': 1308, 'XE': 1309, '\\n3': 1310, '3 ': 1311, 'IM': 1312, 'pk': 1313, 'Eu': 1314, \"',\": 1315, 'z ': 1316, 'EW': 1317, ' X': 1318, 'XI': 1319, 'Rh': 1320, '&c': 1321, 'a$': 1322, '$l': 1323, 'pw': 1324, 'pm': 1325, 'Ic': 1326, 'HI': 1327, 'DA': 1328, 'MU': 1329, '\\nj': 1330, 'PO': 1331, 'IX': 1332, 'RM': 1333, \"'V\": 1334, 'wc': 1335, 'yt': 1336, 'x-': 1337, 'Sk': 1338, 'Dw': 1339, 'IG': 1340, '\\n-': 1341, 'A\\n': 1342, 'Eq': 1343, 'YC': 1344, 'FL': 1345, 'ZE': 1346, 'mr': 1347, 'xl': 1348, 'gp': 1349, 'sj': 1350, 'x?': 1351, 'bn': 1352, 'TR': 1353, 'UD': 1354, 'km': 1355, 'LB': 1356, 'W:': 1357, 'Ig': 1358, 'Ow': 1359, 'nz': 1360, 'Py': 1361, 'yg': 1362, 'c:': 1363, 'Ka': 1364, 'pd': 1365, 'BH': 1366, 'RN': 1367, 'sv': 1368, 'zu': 1369, 'sr': 1370, \"'u\": 1371, 'E.': 1372, 'c?': 1373, 'Io': 1374, 'PT': 1375, 'KA': 1376, 'NS': 1377, \"'k\": 1378, 'Xa': 1379, 'z!': 1380, 'b?': 1381, 'Aj': 1382, 'tb': 1383, \"'N\": 1384, 'bw': 1385, 'ji': 1386, 'IP': 1387, \"E'\": 1388, 'RV': 1389, 'G-': 1390, 'O\\n': 1391, 'NZ': 1392, 'EB': 1393, 'rz': 1394, 'SP': 1395, 'fn': 1396, 'Sy': 1397, 'IB': 1398, 'FE': 1399, '-D': 1400, 'dt': 1401, 'DR': 1402}\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "with open('shakespeare.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Create character to index and index to character mappings\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "printable_chars = tuple(set(string.printable))\n",
    "printable_int2char = dict(enumerate(printable_chars))\n",
    "printable_char2int = {ch:ii for ii,ch in printable_int2char.items()}\n",
    "\n",
    "# Encode by character\n",
    "encoded = np.array([char2int[ch] for ch in text])\n",
    "\n",
    "\n",
    "## Encode the character pairs within the text\n",
    "double_encoded = []\n",
    "double_char2int = {}\n",
    "\n",
    "# Grab all of the character pairs\n",
    "for i in range(len(text) - 1):\n",
    "    pair = text[i:i+2]\n",
    "    if pair not in double_char2int:\n",
    "        # Assign the next number to any new pairs\n",
    "        double_char2int[pair] = len(double_char2int)\n",
    "    # Encode the character pair with its numerical representation\n",
    "    double_encoded.append(double_char2int[pair])\n",
    "    #print(pair, double_char2int[pair])\n",
    "\n",
    "double_int2char = {i: ch for ch, i in double_char2int.items()}\n",
    "double_encoded = np.array(double_encoded)\n",
    "pairs = tuple(set(double_char2int.keys()))\n",
    "\n",
    "\n",
    "total_chars = len(chars)\n",
    "total_pairs = len(pairs)\n",
    "total_labels = 0\n",
    "\n",
    "\n",
    "# We can see that there are 65 different characters within the tinyshakespeare dataset,\n",
    "# far less than the total amount of printable characters,\n",
    "print(total_chars, len(printable_chars))\n",
    "print(char2int)\n",
    "# Similarly, we can see that there are far less unique pairs of characters within the text than are possible.\n",
    "# We will ignore pairs not included (like zz) to save on processing (and these pairs likely would be ignored anyways)\n",
    "# Though this also means that we'll have to be careful if we ever ask this network to produce text from a string \n",
    "# that isn't included in this encoding.  We'll handle that in a function later.\n",
    "print(total_pairs, total_chars**2)\n",
    "print(double_char2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_jnID0Ki1gCx"
   },
   "outputs": [],
   "source": [
    "class CharRNN_normal(torch.nn.Module):\n",
    "    def __init__(self, tokens, n_hidden=500, n_layers=2, batch_size=64, drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "\n",
    "        self.labels = tokens #string.printable\n",
    "        self.printable_chars = printable_chars\n",
    "        self.int2char = dict(enumerate(self.labels))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()} #dict(enumerate(self.printable_chars)).items()\n",
    "        self.output_size = len(self.labels) #len(self.printable_chars)\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(self.output_size, n_hidden, n_layers,\n",
    "                                  dropout=drop_prob, batch_first=True)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(drop_prob)\n",
    "\n",
    "        self.linear = torch.nn.Linear(n_hidden, self.output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # self.lstm's x wants (batch_size, seq_length, total_labels)\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        out = self.dropout(r_output)\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        out = self.linear(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size): #=self.batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        if train_on_gpu:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN_doubleEncode(torch.nn.Module):\n",
    "    def __init__(self, tokens, n_hidden=300, n_layers=2, batch_size=64, drop_prob=0.2, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "\n",
    "        self.labels = tokens\n",
    "        self.double_int2char = double_int2char\n",
    "        self.double_char2int = double_char2int\n",
    "        self.output_size = len(self.labels)\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(self.output_size, n_hidden, n_layers,\n",
    "                                  dropout=drop_prob, batch_first=True)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(drop_prob)\n",
    "\n",
    "        self.linear = torch.nn.Linear(n_hidden, self.output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # self.lstm's x wants (batch_size, seq_length, total_pairs)\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        out = self.dropout(r_output)\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        out = self.linear(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size): #=self.batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        if train_on_gpu:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to one-hot encode the characters\n",
    "def one_hot_encode(arr, n_labels=total_labels):\n",
    "    # arr is shape seq_length, batch_size\n",
    "    arr = arr.transpose(1,0)\n",
    "    oh = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    oh[np.arange(oh.shape[0]), arr.flatten()] = 1\n",
    "    oh = oh.reshape((*arr.shape, n_labels))\n",
    "    return oh.transpose(1,0,2)\n",
    "\n",
    "### Actually, didn't end up using these :( ###\n",
    "# # Function to one-hot encode groups of 2 characters, in attempt to increase contextual range\n",
    "# def one_hot_encode_double(arr, n_labels=total_labels):\n",
    "#     # arr is shape seq_length, batch_size\n",
    "#     arr = arr.transpose(1,0)\n",
    "#     if (arr.shape[1]%2 == 1): arr = np.insert(arr, 0, 5, axis=1) # \" \" is 5, to make the new seq_length even\n",
    "#     new = np.zeros((arr.shape[0], int(arr.shape[1]/2)), dtype=int)\n",
    "#     oh = np.zeros((*(new.shape), n_labels**2), dtype=np.float32) # *(new.shape) here because .shape returns a tuple\n",
    "#     # below only works because arr has been pre-processed to be from min-index to max-index\n",
    "#     for i in range(0, arr.shape[1], 2):\n",
    "#         new[:,int(i/2)] = n_labels*arr[:,i] + arr[:,i+1]\n",
    "#     oh[np.arange(oh.shape[0])[:,None], np.arange(oh.shape[1]), new] = 1\n",
    "#     # oh = oh.reshape((*new.shape, n_labels**2))\n",
    "#     # Ex. oh of n**2 = 9, (batch,seq) [1,2],[3,4],[5,0] would be\n",
    "#     # [[0,1,0,0,0,0,0,0,0], [0,0,1,0,0,0,0,0,0]\n",
    "#     #  [0,0,0,1,0,0,0,0,0], [0,0,0,0,1,0,0,0,0]\n",
    "#     #  [0,0,0,0,0,1,0,0,0], [1,0,0,0,0,0,0,0,0]]\n",
    "    \n",
    "#     # returns oh of shape seq_length, batch_size, n_labels**2\n",
    "#     return oh.transpose(1,0,2)\n",
    "\n",
    "# def target_to_double(target_arr, n_labels=total_labels):\n",
    "#     target_arr = target_arr.transpose(1,0)\n",
    "#     if (target_arr.shape[1]%2 == 1): target_arr = np.insert(target_arr, 0, 76, axis=1) # \" \" is 5, to make the new seq_length even\n",
    "#     converted = np.zeros((target_arr.shape[0], int(target_arr.shape[1]/2)), dtype=int)\n",
    "#     for i in range(0, target_arr.shape[1], 2):\n",
    "#         converted[:,int(i/2)] = n_labels*target_arr[:,i] + target_arr[:,i+1]\n",
    "#     return converted.transpose(1,0)\n",
    "\n",
    "# def double_to_char(doubled, n_labels=total_labels):\n",
    "#     doubled = doubled.transpose(1,0)\n",
    "#     reverted = np.zeros((doubled.shape[0], int(doubled.shape[1]*2)), dtype=int)\n",
    "#     for i in range(0, doubled.shape[1]):\n",
    "#         reverted[:, i+1] = doubled[:, i] % n_labels\n",
    "#         reverted[:, i] = (doubled[:, i] - reverted[:, i+1]) / n_labels\n",
    "#     return reverted.transpose(1,0)## Encoding functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JsogAoRn4nPM"
   },
   "outputs": [],
   "source": [
    "# Get a random sequence of the Shakespeare dataset.\n",
    "def get_random_seq_and_target(arr, seq_length):\n",
    "    start_index = random.randint(0, len(arr) - seq_length - 1)\n",
    "    end_index   = start_index + seq_length + 1 - 1\n",
    "    return arr[start_index:end_index], arr[start_index+1:end_index+1]\n",
    "\n",
    "# Get a random paired sequence of the Shakespeare dataset.\n",
    "# Iterates 2 indexes at a time\n",
    "def pair_get_random_seq_and_target(arr, seq_length):\n",
    "    increment = 2\n",
    "    start_index = random.randint(0, len(arr) - 2*seq_length - increment)\n",
    "    end_index   = start_index + 2*seq_length + increment - increment\n",
    "    return arr[start_index:end_index:2], arr[start_index+increment:end_index+increment:2]\n",
    "### WORTH PAYING ATTENTION TO INCREMENT OF TARGET - +1 vs +2 could make a huge difference here\n",
    "\n",
    "\n",
    "def get_batches(arr, batch_size, seq_length, batches_per_iter, pair_mode = False, n_labels=total_labels):\n",
    "    '''Arguments\n",
    "       ---------\n",
    "       arr: Total char array to make batches from, 1-D\n",
    "       batch_size: the number of sequences per batch\n",
    "       seq_length: number of encoded chars per sequence\n",
    "       batches_per_iter: how many sets for batches per iter/epoch\n",
    "       pair_mode: whether to collect from double_encode or not\n",
    "       n_labels: the total number of possible labels\n",
    "    '''\n",
    "\n",
    "    # We want batch to be seq_length,batch_size (128,64)\n",
    "\n",
    "    batch_size_total = batch_size * seq_length\n",
    "    batch = np.zeros((seq_length, batch_size), dtype=int)\n",
    "    target = np.zeros((seq_length, batch_size), dtype=int)\n",
    "    \n",
    "    for b in range(0, batches_per_iter):\n",
    "        # iterate through the array, one random sequence at a time\n",
    "        for n in range(0, batch_size):\n",
    "            if pair_mode:\n",
    "                batch[:,n], target[:,n] = pair_get_random_seq_and_target(arr, seq_length)\n",
    "            else:\n",
    "                batch[:,n], target[:,n] = get_random_seq_and_target(arr, seq_length)\n",
    "        yield torch.from_numpy(one_hot_encode(batch, n_labels=n_labels).transpose(1,0,2)), torch.from_numpy(target.transpose(1,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "R64yCm3D5jZf"
   },
   "outputs": [],
   "source": [
    "def train_step(net, opt, loss_func, batch_size, input, target):\n",
    "    # Initialize hidden state and gradients.\n",
    "    hidden = net.init_hidden(batch_size)\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    # Forward pass.\n",
    "    output, hidden = net(input, hidden)\n",
    "    \n",
    "    # Compute loss. Flatten output and target tensors and compute cross-entropy.\n",
    "    loss = loss_func(output.reshape(-1, net.output_size), target.reshape(-1))\n",
    "\n",
    "    # Backward pass and optimization.\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "def train(net, data, pair_mode = False, epochs=10, batch_size=64, seq_length=128, lr=0.001, clip=5, val_frac=0.1, print_every=100):\n",
    "    ''' Training a network\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "\n",
    "        net: CharRNN_normal network\n",
    "        data: text data to train the network\n",
    "        pair_mode: whether to refer to double_encode or not\n",
    "        epochs: Number of epochs.iters to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "\n",
    "    '''\n",
    "    all_losses = []\n",
    "    loss_sum   = 0\n",
    "    \n",
    "    net.train()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "\n",
    "    counter = 0\n",
    "    'THIS AFFECTS INPUT SIZE'\n",
    "    n_labels = len(net.labels)\n",
    "    batches_per_iter=1000\n",
    "    \n",
    "    \n",
    "    start_train = timeit.default_timer()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for input, target in get_batches(data, batch_size, seq_length, batches_per_iter, pair_mode=pair_mode, n_labels=n_labels):\n",
    "            input, target = input.to(device), target.to(device) # Move to GPU memory.\n",
    "            #print(\"input+target shape:\", input.shape, target.shape)\n",
    "            #input+target shape: torch.Size([128, 64, 65]) torch.Size([128, 64])\n",
    "            \n",
    "            loss      = train_step(net, opt, loss_func, batch_size, input, target)   # Calculate the loss.\n",
    "            loss_sum += loss                                  # Accumulate the loss.\n",
    "            \n",
    "            counter += 1\n",
    "            # Print the log.\n",
    "            if counter % print_every == print_every - 1:\n",
    "                print('iter:{}/{} loss:{}'.format(counter+1, batches_per_iter, loss_sum / print_every))\n",
    "                #print('generated sequence: {}\\n'.format(eval_step(net)))\n",
    "\n",
    "                # Track the loss.\n",
    "                all_losses.append(loss_sum / print_every)\n",
    "                loss_sum = 0\n",
    "        \n",
    "        val_h = net.init_hidden(batch_size)\n",
    "        val_losses = []\n",
    "        net.eval()\n",
    "        for input, target in get_batches(val_data, batch_size, seq_length, batches_per_iter=5, pair_mode=pair_mode, n_labels=n_labels):\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "            if(train_on_gpu):\n",
    "                input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            output, val_h = net(input, val_h)\n",
    "            val_loss = loss_func(output.reshape(-1, net.output_size), target.reshape(-1))\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "        net.train() # reset to train mode after iterationg through validation data\n",
    "        print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "              \"Step: {}...\".format(counter),\n",
    "              \"Loss: {:.4f}...\".format(loss),\n",
    "              \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
    "\n",
    "    \n",
    "    end_train = timeit.default_timer()\n",
    "    print (\"Training time elapsed:\", end_train - start_train, \"s\")\n",
    "    \n",
    "    return loss_sum, all_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jWDaVojH162C",
    "outputId": "413a59af-63ff-4272-c4b9-6ba57b6332ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN_normal(\n",
      "  (lstm): LSTM(65, 300, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (linear): Linear(in_features=300, out_features=65, bias=True)\n",
      ")\n",
      "1182365\n",
      "CharRNN_doubleEncode(\n",
      "  (lstm): LSTM(1403, 170, num_layers=2, batch_first=True, dropout=0.4)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (linear): Linear(in_features=170, out_features=1403, bias=True)\n",
      ")\n",
      "1543473\n",
      "iter:100/1000 loss:5.947876739501953\n",
      "iter:200/1000 loss:5.78947594165802\n",
      "iter:300/1000 loss:5.6466197538375855\n",
      "iter:400/1000 loss:5.423914928436279\n",
      "iter:500/1000 loss:5.18526487827301\n",
      "iter:600/1000 loss:4.890898160934448\n",
      "iter:700/1000 loss:4.64752426147461\n",
      "iter:800/1000 loss:4.489025683403015\n",
      "iter:900/1000 loss:4.373026790618897\n",
      "iter:1000/1000 loss:4.275673508644104\n",
      "Epoch: 1/15... Step: 1000... Loss: 4.1895... Val Loss: 4.2492\n",
      "iter:1100/1000 loss:4.19410397529602\n",
      "iter:1200/1000 loss:4.121131644248963\n",
      "iter:1300/1000 loss:4.067677206993103\n",
      "iter:1400/1000 loss:4.010718848705292\n",
      "iter:1500/1000 loss:3.959303297996521\n",
      "iter:1600/1000 loss:3.9236297750473024\n",
      "iter:1700/1000 loss:3.8683677411079405\n",
      "iter:1800/1000 loss:3.8415430855751036\n",
      "iter:1900/1000 loss:3.8052703666687013\n",
      "iter:2000/1000 loss:3.779603350162506\n",
      "Epoch: 2/15... Step: 2000... Loss: 3.7396... Val Loss: 3.8924\n",
      "iter:2100/1000 loss:3.752593734264374\n",
      "iter:2200/1000 loss:3.718178174495697\n",
      "iter:2300/1000 loss:3.6964071655273436\n",
      "iter:2400/1000 loss:3.664490215778351\n",
      "iter:2500/1000 loss:3.6481792664527894\n",
      "iter:2600/1000 loss:3.6239434266090393\n",
      "iter:2700/1000 loss:3.6020212745666504\n",
      "iter:2800/1000 loss:3.5782951307296753\n",
      "iter:2900/1000 loss:3.5590379500389098\n",
      "iter:3000/1000 loss:3.5359279537200927\n",
      "Epoch: 3/15... Step: 3000... Loss: 3.5890... Val Loss: 3.7074\n",
      "iter:3100/1000 loss:3.523134891986847\n",
      "iter:3200/1000 loss:3.5075591707229616\n",
      "iter:3300/1000 loss:3.480017037391663\n",
      "iter:3400/1000 loss:3.46718444108963\n",
      "iter:3500/1000 loss:3.4537304425239563\n",
      "iter:3600/1000 loss:3.4351436495780945\n",
      "iter:3700/1000 loss:3.421983425617218\n",
      "iter:3800/1000 loss:3.410655343532562\n",
      "iter:3900/1000 loss:3.3906637740135195\n",
      "iter:4000/1000 loss:3.3810869121551512\n",
      "Epoch: 4/15... Step: 4000... Loss: 3.3564... Val Loss: 3.5225\n",
      "iter:4100/1000 loss:3.3688596081733704\n",
      "iter:4200/1000 loss:3.3581043314933776\n",
      "iter:4300/1000 loss:3.348758261203766\n",
      "iter:4400/1000 loss:3.327229323387146\n",
      "iter:4500/1000 loss:3.3221006560325623\n",
      "iter:4600/1000 loss:3.304406087398529\n",
      "iter:4700/1000 loss:3.2929674696922304\n",
      "iter:4800/1000 loss:3.282640864849091\n",
      "iter:4900/1000 loss:3.2729086208343507\n",
      "iter:5000/1000 loss:3.2676559495925903\n",
      "Epoch: 5/15... Step: 5000... Loss: 3.2784... Val Loss: 3.4403\n",
      "iter:5100/1000 loss:3.2637174940109253\n",
      "iter:5200/1000 loss:3.2539688515663148\n",
      "iter:5300/1000 loss:3.244811723232269\n",
      "iter:5400/1000 loss:3.230431776046753\n",
      "iter:5500/1000 loss:3.2234679675102234\n",
      "iter:5600/1000 loss:3.2161245346069336\n",
      "iter:5700/1000 loss:3.2032910585403442\n",
      "iter:5800/1000 loss:3.198411111831665\n",
      "iter:5900/1000 loss:3.1952704405784607\n",
      "iter:6000/1000 loss:3.1874838733673094\n",
      "Epoch: 6/15... Step: 6000... Loss: 3.1809... Val Loss: 3.4178\n",
      "iter:6100/1000 loss:3.1845348191261293\n",
      "iter:6200/1000 loss:3.1714196681976317\n",
      "iter:6300/1000 loss:3.163205201625824\n",
      "iter:6400/1000 loss:3.162433145046234\n",
      "iter:6500/1000 loss:3.143737757205963\n",
      "iter:6600/1000 loss:3.142044105529785\n",
      "iter:6700/1000 loss:3.1419920778274535\n",
      "iter:6800/1000 loss:3.123260896205902\n",
      "iter:6900/1000 loss:3.118013958930969\n",
      "iter:7000/1000 loss:3.1124373269081116\n",
      "Epoch: 7/15... Step: 7000... Loss: 3.0490... Val Loss: 3.3735\n",
      "iter:7100/1000 loss:3.109964349269867\n",
      "iter:7200/1000 loss:3.1137514996528624\n",
      "iter:7300/1000 loss:3.098339653015137\n",
      "iter:7400/1000 loss:3.0959733891487122\n",
      "iter:7500/1000 loss:3.0861156678199766\n",
      "iter:7600/1000 loss:3.0771051025390626\n",
      "iter:7700/1000 loss:3.07514276266098\n",
      "iter:7800/1000 loss:3.0707735204696656\n",
      "iter:7900/1000 loss:3.058121774196625\n",
      "iter:8000/1000 loss:3.063342323303223\n",
      "Epoch: 8/15... Step: 8000... Loss: 3.0930... Val Loss: 3.3672\n",
      "iter:8100/1000 loss:3.0557980799674986\n",
      "iter:8200/1000 loss:3.0525305247306824\n",
      "iter:8300/1000 loss:3.049157407283783\n",
      "iter:8400/1000 loss:3.043613922595978\n",
      "iter:8500/1000 loss:3.0446281147003176\n",
      "iter:8600/1000 loss:3.0354717564582825\n",
      "iter:8700/1000 loss:3.0305562615394592\n",
      "iter:8800/1000 loss:3.023736057281494\n",
      "iter:8900/1000 loss:3.0209884357452395\n",
      "iter:9000/1000 loss:3.015077428817749\n",
      "Epoch: 9/15... Step: 9000... Loss: 2.9922... Val Loss: 3.3879\n",
      "iter:9100/1000 loss:3.0137997531890868\n",
      "iter:9200/1000 loss:3.006851923465729\n",
      "iter:9300/1000 loss:3.004854621887207\n",
      "iter:9400/1000 loss:2.9946333837509154\n",
      "iter:9500/1000 loss:2.993416378498077\n",
      "iter:9600/1000 loss:2.988307569026947\n",
      "iter:9700/1000 loss:2.987830095291138\n",
      "iter:9800/1000 loss:2.9844371509552\n",
      "iter:9900/1000 loss:2.9813866782188416\n",
      "iter:10000/1000 loss:2.975900933742523\n",
      "Epoch: 10/15... Step: 10000... Loss: 2.9828... Val Loss: 3.3597\n",
      "iter:10100/1000 loss:2.9704545974731444\n",
      "iter:10200/1000 loss:2.9632762026786805\n",
      "iter:10300/1000 loss:2.9647942352294923\n",
      "iter:10400/1000 loss:2.961140866279602\n",
      "iter:10500/1000 loss:2.962383096218109\n",
      "iter:10600/1000 loss:2.951757254600525\n",
      "iter:10700/1000 loss:2.9504204630851745\n",
      "iter:10800/1000 loss:2.947953789234161\n",
      "iter:10900/1000 loss:2.9462541842460634\n",
      "iter:11000/1000 loss:2.939019603729248\n",
      "Epoch: 11/15... Step: 11000... Loss: 3.0090... Val Loss: 3.3943\n",
      "iter:11100/1000 loss:2.9402972507476806\n",
      "iter:11200/1000 loss:2.9401915192604067\n",
      "iter:11300/1000 loss:2.93427595615387\n",
      "iter:11400/1000 loss:2.936025466918945\n",
      "iter:11500/1000 loss:2.931793351173401\n",
      "iter:11600/1000 loss:2.9263034772872927\n",
      "iter:11700/1000 loss:2.925570809841156\n",
      "iter:11800/1000 loss:2.9210111474990845\n",
      "iter:11900/1000 loss:2.9174337148666383\n",
      "iter:12000/1000 loss:2.9145729446411135\n",
      "Epoch: 12/15... Step: 12000... Loss: 2.8739... Val Loss: 3.3678\n",
      "iter:12100/1000 loss:2.9169477248191833\n",
      "iter:12200/1000 loss:2.9082541179656984\n",
      "iter:12300/1000 loss:2.91141676902771\n",
      "iter:12400/1000 loss:2.905283591747284\n",
      "iter:12500/1000 loss:2.9020523500442503\n",
      "iter:12600/1000 loss:2.8975374722480773\n",
      "iter:12700/1000 loss:2.8981248188018798\n",
      "iter:12800/1000 loss:2.8989762258529663\n",
      "iter:12900/1000 loss:2.8928800129890444\n",
      "iter:13000/1000 loss:2.882714374065399\n",
      "Epoch: 13/15... Step: 13000... Loss: 2.8934... Val Loss: 3.4299\n",
      "iter:13100/1000 loss:2.8825361204147337\n",
      "iter:13200/1000 loss:2.8828599095344543\n",
      "iter:13300/1000 loss:2.8799564146995547\n",
      "iter:13400/1000 loss:2.8758748173713684\n",
      "iter:13500/1000 loss:2.871677265167236\n",
      "iter:13600/1000 loss:2.878352527618408\n",
      "iter:13700/1000 loss:2.8729805874824526\n",
      "iter:13800/1000 loss:2.872836105823517\n",
      "iter:13900/1000 loss:2.8704964756965636\n",
      "iter:14000/1000 loss:2.8763947081565857\n",
      "Epoch: 14/15... Step: 14000... Loss: 2.8671... Val Loss: 3.3889\n",
      "iter:14100/1000 loss:2.8692031049728395\n",
      "iter:14200/1000 loss:2.8675423288345336\n",
      "iter:14300/1000 loss:2.8607143855094908\n",
      "iter:14400/1000 loss:2.8657231640815737\n",
      "iter:14500/1000 loss:2.8520669388771056\n",
      "iter:14600/1000 loss:2.8524273467063903\n",
      "iter:14700/1000 loss:2.8519373989105223\n",
      "iter:14800/1000 loss:2.850806772708893\n",
      "iter:14900/1000 loss:2.8433181405067445\n",
      "iter:15000/1000 loss:2.849503936767578\n",
      "Epoch: 15/15... Step: 15000... Loss: 2.8730... Val Loss: 3.4094\n",
      "Training time elapsed: 713.2395419729874 s\n"
     ]
    }
   ],
   "source": [
    "### Normal CharRNN ###\n",
    "\n",
    "n_hidden = 300\n",
    "n_layers = 2\n",
    "dropout = 0.5\n",
    "batch_size = 64\n",
    "seq_length = 128\n",
    "n_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "total_labels = total_chars\n",
    "double = False\n",
    "\n",
    "net = CharRNN_normal(chars, n_hidden, n_layers)\n",
    "print(net)\n",
    "print(sum(p.numel() for p in net.parameters()))\n",
    "# # train the model\n",
    "# loss_sum, all_losses = train(net, encoded, epochs=n_epochs, batch_size=batch_size, \n",
    "#                              seq_length=seq_length, lr=learning_rate, print_every=500)\n",
    "\n",
    "\n",
    "### Double Encoded CharRNN ###\n",
    "\n",
    "n_hidden = 170\n",
    "n_layers = 2\n",
    "dropout = 0.4\n",
    "batch_size = 64\n",
    "seq_length = 128\n",
    "n_epochs = 15\n",
    "learning_rate = 0.001\n",
    "\n",
    "total_labels = total_pairs\n",
    "double = True\n",
    "\n",
    "netDouble = CharRNN_doubleEncode(pairs, n_hidden, n_layers, drop_prob=dropout)\n",
    "print(netDouble)\n",
    "print(sum(p.numel() for p in netDouble.parameters()))\n",
    "# train the model\n",
    "loss_sum, all_losses = train(netDouble, double_encoded, pair_mode = True, epochs=n_epochs, batch_size=batch_size, \n",
    "                             seq_length=seq_length, lr=learning_rate, print_every=100)\n",
    "\n",
    "\n",
    "# torch.save(net.state_dict(), 'SequenceRNN_params_ -layers h- .pth')\n",
    "# with open('SequenceRNN_lossList_ -layers h- _loss- .pkl', 'wb') as file:\n",
    "#     pickle.dump(all_losses, file)\n",
    "# torch.save(netDouble.state_dict(), 'DoubleRNN_params_standard.pth')\n",
    "# with open('DoubleRNN_lossList_standard.pkl', 'wb') as file:\n",
    "#     pickle.dump(all_losses, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('iters')\n",
    "plt.ylabel('loss')\n",
    "plt.plot([loss for loss in all_losses])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sum(p.numel() for p in net.parameters()))\n",
    "#print(sum(p.numel() for p in netDouble.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Single encode ###\n",
    "## 3 layers, 300 hidden, dropout 0.5, 0.001 lr, 20 epochs\n",
    "#  909s train time, 1.271 train loss, 1.609 val loss\n",
    "## blah\n",
    "#  blah\n",
    "\n",
    "### Double encode ###\n",
    "## 2 layers, 300 hidden, dropout 0.2, 0.001 lr, 5 epochs\n",
    "# 275s train time, 1.433 train lost, 1.694 val loss\n",
    "# words are not complete though\n",
    "## 3 layers, 400 hidden, dropout 0.3, 0,001 lr, 25 epochs, seq len 200\n",
    "# 4281 train time, 1.7558 train loss, 4.803 val loss\n",
    "# what is interesting here is that loss is way high, but the words are a lot more robust, and you can\n",
    "# even sense some emotion coming from responses... like romeo sounds sad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "9PYIAiyK1--X"
   },
   "outputs": [],
   "source": [
    "# Evaluation step function.\n",
    "def eval_step(net, init_seq='W', predicted_len=100, eval_batch_size=3):\n",
    "    # Enter eval mode\n",
    "    net.eval()\n",
    "    \n",
    "    # Initialize the hidden state, input and the predicted sequence.\n",
    "    hidden        = net.init_hidden(eval_batch_size)\n",
    "    encoded_seq = np.array([char2int[ch] for ch in init_seq])\n",
    "    init_input    = torch.from_numpy(one_hot_encode(np.array(([encoded_seq]*eval_batch_size))\n",
    "                                    .reshape(eval_batch_size, len(init_seq)).transpose(1,0)).transpose(1,0,2)).to(device)\n",
    "    predicted_seq = np.array(([ch for ch in init_seq]*eval_batch_size)).reshape(eval_batch_size, len(init_seq))\n",
    "    # predicted_seq.shape = (batch, seq, 0)\n",
    "    \n",
    "    # Use initial string to \"build up\" hidden state.\n",
    "    #print(init_input[:,0,:].unsqueeze(1).shape)\n",
    "    for t in range(len(init_seq) - 1):\n",
    "        output, hidden = net(init_input[:,t,:].unsqueeze(1), hidden) # input shape (batch_size, seq_length, n_chars)\n",
    "        \n",
    "    # Set current input as the last character of the initial string.\n",
    "    input = init_input[:,-1,:].unsqueeze(1)\n",
    "    \n",
    "    # Predict more characters after the initial string.\n",
    "    for t in range(predicted_len):\n",
    "        # Get the current output and hidden state.\n",
    "        output, hidden = net(input, hidden)\n",
    "        \n",
    "        # Sample from the output as a multinomial distribution.\n",
    "        predicted_index = tuple(torch.multinomial(output[:, :].exp(), 1)[:].flatten().tolist())\n",
    "        #predicted_index = torch.multinomial(output[:, :].exp(), 1)[:].numpy()\n",
    "        \n",
    "        # Add predicted character to the sequence and use it as next input.\n",
    "        predicted_chars  = [int2char[i] for i in predicted_index]\n",
    "        predicted_seq = np.concatenate((predicted_seq, np.expand_dims(predicted_chars, axis=1)), axis=1)\n",
    "\n",
    "        \n",
    "        # Use the predicted character to generate the input of next round.\n",
    "        input = torch.from_numpy(one_hot_encode(np.array(predicted_index)\n",
    "                                .reshape(1, eval_batch_size)).transpose(1,0,2)).to(device)\n",
    "\n",
    "    return [''.join(row) for row in predicted_seq]\n",
    "\n",
    "\n",
    "\n",
    "# Pairwise evaluation step function.\n",
    "# with iter=1, it produces staggered language... like double letters\n",
    "def double_eval_step(net, init_seq='Wh', predicted_len=100, eval_batch_size=3):\n",
    "    # Enter eval mode\n",
    "    net.eval()\n",
    "    \n",
    "    init_seq = [init_seq[i:i+2] for i in range(0, len(init_seq), 2)]\n",
    "    \n",
    "    # Initialize the hidden state, input and the predicted sequence.\n",
    "    hidden        = net.init_hidden(eval_batch_size)\n",
    "    encoded_seq = np.array([double_char2int[ch] for ch in init_seq])\n",
    "    init_input    = torch.from_numpy(one_hot_encode(np.array(([encoded_seq]*eval_batch_size))\n",
    "                                    .reshape(eval_batch_size, len(init_seq)).transpose(1,0), n_labels=len(net.labels)).transpose(1,0,2)).to(device)\n",
    "    predicted_seq = np.array(([ch for ch in init_seq]*eval_batch_size)).reshape(eval_batch_size, len(init_seq))\n",
    "    # predicted_seq.shape = (batch, seq, 0)\n",
    "    \n",
    "    # Use initial string to \"build up\" hidden state.\n",
    "    #print(init_input[:,0,:].unsqueeze(1).shape)\n",
    "    for t in range(len(init_seq) - 1):\n",
    "        output, hidden = net(init_input[:,t,:].unsqueeze(1), hidden) # input shape (batch_size, seq_length, n_chars)\n",
    "        \n",
    "    # Set current input as the last character of the initial string.\n",
    "    input = init_input[:,-1,:].unsqueeze(1)\n",
    "    \n",
    "    counter=0\n",
    "    # Predict more characters after the initial string.\n",
    "    for t in range(predicted_len):\n",
    "        counter += 1\n",
    "        # Get the current output and hidden state.\n",
    "        output, hidden = net(input, hidden)\n",
    "        \n",
    "        # Sample from the output as a multinomial distribution.\n",
    "        predicted_index = tuple(torch.multinomial(output[:, :].exp(), 1)[:].flatten().tolist())\n",
    "        #predicted_index = torch.multinomial(output[:, :].exp(), 1)[:].numpy()\n",
    "        \n",
    "        # Add predicted character to the sequence and use it as next input.\n",
    "        predicted_chars  = [double_int2char[i] for i in predicted_index]\n",
    "        'v TO AVOID DOUBLING PROBLEM, LIKELY FROM ITERATOR IN THE RANDOM SEQUENCE FUNCTION'\n",
    "        #if counter%2 == 0:\n",
    "        predicted_seq = np.concatenate((predicted_seq, np.expand_dims(predicted_chars, axis=1)), axis=1)\n",
    "\n",
    "        \n",
    "        # Use the predicted character to generate the input of next round.\n",
    "        input = torch.from_numpy(one_hot_encode(np.array(predicted_index)\n",
    "                                .reshape(1, eval_batch_size), n_labels=len(net.labels)).transpose(1,0,2)).to(device)\n",
    "\n",
    "    return [''.join(row) for row in predicted_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whese shree, lad'd I am sons: kill, hearing\n",
      "Hase make him with wash I'll came unwe accanting\n",
      "of that best; and where thou do done, who noblest.\n",
      "When this will all.\n",
      "\n",
      "ROMEO:\n",
      "O, thank you, remy left even as hold\n",
      "joy faselood, but fothe Edward's thy flopers, speak, end.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Peace!\n",
      "\n",
      "SAMPSON:\n",
      "But say you proported may she is hear such see on.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "O here not the good senfore back with thy dromely,\n",
      "A Envish they thou cilderish but the see;\n",
      "For, ho! doth be his meet. By the bolinasion,\n",
      "Well him for hear fow any cous of show.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Now larh, this reeducill'd and all on\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Whom he in thy liberty,\n",
      "Fompone their hile-hand might as I poul must stroat\n",
      "The wretched to if him, in a strove had knights,\n",
      "I thinks and wolly forth, some hand trouding feed\n",
      "the souls for yours: with stumble; that I proceed to your\n",
      "provipel, for this finger; where they strange mooth much lords.\n",
      "Put our soldiers as so.\n",
      "\n",
      "CORIOLANUS:\n",
      "But sost, the prince? Here is bewromet especice,\n",
      "He nods sate the staming and diddilled that well.\n",
      "Keep abuse, as the set.\n",
      "\n",
      "First Lord:\n",
      "If death for in my glets day, and sir o' the dover: he may have on earth,\n",
      "Too be in thy tuck and here by thy guilt:\n",
      "Brother's sees, \n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Which lord to pleein thou humbly'st thee;\n",
      "Be Trakeies, efter clatch me again, the swifts,\n",
      "It shall see thee grace partian, to becondrace,\n",
      "That born the king appirious destance\n",
      "To say you should not be prison, sits' purtence.\n",
      "\n",
      "HASTINGS:\n",
      "I'll help your knowledge, good! O malice?\n",
      "\n",
      "Second:\n",
      "We how reneect myself, what nay, and they know\n",
      "And prince a way, I love with twill cribs,\n",
      "To betime thee but with sir! but's not hast,\n",
      "And not hone there minute and puning me.\n",
      "\n",
      "LUCIO:\n",
      "\n",
      "COMINIUS:\n",
      "Our lord here, sir, 'pardod, as this's blengest more. 'Tough thy\n",
      "treen let's proud give me waste scuted.\n",
      "\n",
      "CORIOLANUS:\n",
      "Br\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "When as he every heart's find to his odds\n",
      "That was I tale, that shall are Volast's another\n",
      "Bad hath not abele to any thank thou foul.\n",
      "I say you know me little mow! where's good poor they;\n",
      "I go.\n",
      "\n",
      "CAMILLO:\n",
      "Let he dead. What warrant our breath.\n",
      "\n",
      "DUKE OF YORK:\n",
      "Now, not a sisters sir, and before: you she hunced from\n",
      "the core to be, cenfice behind and some ear to\n",
      "befathin always son like whose that is you are gund;\n",
      "Would be well not valeon'd my fortune sar'd\n",
      "Upon me than there an!\n",
      "\n",
      "BRUTUS:\n",
      "Camedy it.\n",
      "Each Camuscause, I well not diglerder\n",
      "You will the bokess an the friends, why neither\n",
      "We slamber'd in \n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Why best, first an recowed thing\n",
      "more of the patience.\n",
      "\n",
      "VALERIA:\n",
      "Son, as steel to use, Service shall beseech\n",
      "you.\n",
      "\n",
      "POLIXENES:\n",
      "Well, madam, there be must I for so said\n",
      "I foil to Herfare you, to make for yourself.\n",
      "But she could have the hath beneldly, I or, could be\n",
      "comforted ore about your vigipent of I chause!\n",
      "\n",
      "MENENIUS:\n",
      "O, 'Cair welcomes; Juliet!\n",
      "\n",
      "CLARENCE:\n",
      "Why, he's never to be known to-sences\n",
      "Bould have her boy catted an any majesty.\n",
      "And all of my side love, the heart and me\n",
      "Of show him father, face to-morrow adtine,\n",
      "I do the back for a stall deirt of dakes.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "You wilt this of\n",
      "\n",
      "-------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batches = 5\n",
    "#generated_seqs = eval_step(net, pair_mode = double, init_seq='ROMEO', predicted_len=200, eval_batch_size=batches)\n",
    "generated_seqs = double_eval_step(netDouble, init_seq='Wh', predicted_len=300, eval_batch_size=batches)\n",
    "\n",
    "\n",
    "for i in range(batches):\n",
    "    print(generated_seqs[i])\n",
    "    print(\"\\n-------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "9bGGKzaf2Dpb"
   },
   "outputs": [],
   "source": [
    "# def predict(net, char, h=None, top_k=None):\n",
    "#         ''' Given a character, predict the next character.\n",
    "#             Returns the predicted character and the hidden state.\n",
    "#         '''\n",
    "\n",
    "#         # tensor inputs\n",
    "#         x = np.array([[net.char2int[char]]])\n",
    "#         x = one_hot_encode(x, len(net.chars))\n",
    "#         inputs = torch.from_numpy(x)\n",
    "\n",
    "#         if(train_on_gpu):\n",
    "#             inputs = inputs.cuda()\n",
    "\n",
    "#         # detach hidden state from history\n",
    "#         h = tuple([each.data for each in h])\n",
    "#         # get the output of the model\n",
    "#         out, h = net(inputs, h)\n",
    "\n",
    "#         # get the character probabilities\n",
    "#         p = F.softmax(out, dim=1).data\n",
    "#         if(train_on_gpu):\n",
    "#             p = p.cpu() # move to cpu\n",
    "\n",
    "#         # get top characters\n",
    "#         if top_k is None:\n",
    "#             top_ch = np.arange(len(net.chars))\n",
    "#         else:\n",
    "#             p, top_ch = p.topk(top_k)\n",
    "#             top_ch = top_ch.numpy().squeeze()\n",
    "\n",
    "#         # select the likely next character with some element of randomness\n",
    "#         p = p.numpy().squeeze()\n",
    "#         char = np.random.choice(top_ch, p=p/p.sum())\n",
    "\n",
    "#         # return the encoded value of the predicted char and the hidden state\n",
    "#         return net.int2char[char], h\n",
    "\n",
    "# def sample(net, size, prime='The', top_k=None):\n",
    "\n",
    "#     if(train_on_gpu):\n",
    "#         net.cuda()\n",
    "#     else:\n",
    "#         net.cpu()\n",
    "\n",
    "#     net.eval() # eval mode\n",
    "\n",
    "#     # First off, run through the prime characters\n",
    "#     chars = [ch for ch in prime]\n",
    "#     h = net.init_hidden(1)\n",
    "#     for ch in prime:\n",
    "#         char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "#     chars.append(char)\n",
    "\n",
    "#     # Now pass in the previous character and get a new one\n",
    "#     for ii in range(size):\n",
    "#         char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "#         chars.append(char)\n",
    "\n",
    "#     return ''.join(chars)\n",
    "\n",
    "# print(sample(net, 1000, prime='JULIET', top_k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nme we shall not like.\\n\\nESCALUS:\\nAnd we do proud? Where is he cames.\\n\\nLUCIO:\\nBecome by Saint in his boy, being modest:\\nMy noble lords have last to lose his own,\\nThe love be mew'd in such a dudd of mocker.\\n\\nQUEEN MARGARET:\\nNurserves upon some paly, look at his sil.\\n\\nEDWARD:\\nClaimio is will do this white fall of death.\\n\\nKING RICHARD III:\\nMy lord, he charge thee, tender may in most,\\nWhom I find poor thrive gives to him my love.\\n\\nWARWICK:\\nO maline that fellow, if a begin for me!\\n\\nQUEEN MARGARET:\\nThat will not be mine houglips and tell it:\\nRa' Henry, let us well devilling made\\nA great sise, by the oop\\n\\n-------------------------------------\\n\\nme possess you of things apprehends.\\n\\nWARWICK:\\nHow now! what news?\\n\\nMessenger:\\nMy gracious lord,\\nThat there do his thing that only live.\\n\\nHENRY BOLINGBROKE:\\nYou bestrick, and reason'd mensor--\\nWas much with the impostable face!\\n\\nBENVOLIO:\\nAy, but this buried.\\n\\nPERDITA:\\nYou were command.\\n\\nCLIFFORD:\\nMethor; and the rest is raskind; and I\\nshall away thy sirk, sweet belly.\\n\\nROMEO:\\nI know thy unk to him, we will not\\nI will be vow'd, and 'tis time, thou wilt thou not\\nEscrees though, who I know\\nTo light our lobss, and he dishonour'd blood.\\nIs hewn, Amal, More out or losuliess that he be\\nThat a wound ou\\n\\n-------------------------------------\\n\\nmel'd many condemned shame:\\nReproach false which each must be king, with raidings\\nShall never bonge for the swifter of this worse.\\n\\nBALTHASAR:\\nA brother's traitors, of me, I'll not seen,\\nWith us hence spend can with are quive.\\n\\nBENVOLIO:\\nAnd I love as to be one with recimate\\nAnd all their world, but tyrants for our angs\\nShall stop ord should have gives but their\\nAnd make masquers to the sworch of with a league\\nAnd with me by any clain 'em: I must be\\nTo use my will the people dream, you say:\\nI dreamt and bring him by my occenather.\\n\\nDUKE OF AUMERLE:\\nEven where I bear your shoulds nor we will well\\n\\n-------------------------------------\\n\\nmepher.\\n\\nISABELLA:\\nTrue from his face.\\n\\nPAULINA:\\nHe's dead, which tost thou will do't,\\nWhile these touchesment a vain: here's the moon\\nLike the worse of yours?\\n\\nVOLUMNIA:\\nShe is have see,\\nThe lead that cannot leave.\\n\\nMARIANA:\\nO that I am sure,\\nAnd he hath not been abuse that things to-day\\nFor what the sunsty say to say they from me;\\nAnd when your sovereign king, will sate thee;\\nOur lords of great commanding and my way;\\nTo it, and 'tis unhappy way in arms.\\n\\nGLOUCESTER:\\nThis follow my tongue like a love that she know\\nThou grows thy wither invoctions them.\\n\\nKING EDWARD IV:\\nAy, nor night belormer ha\\n\\n-------------------------------------\\n\\nme: speak?\\n\\nMENENIUS:\\nYou are mine own credively, and now of revenge:\\nI will be heard with strong come basted axe.\\n\\nSecond Murdeo;\\nBelike, disconore,--\\n\\nAUTOLYCUS:\\nEven behaps, my lord! I have piece up to all\\nFirst, not we look'd him: he is become he hope\\nthat woe and were the porely change you what\\nthey are.\\n\\nMOPSA:\\nO, they are in a most velconceley, hast\\nyou, lords? COMhaRD:\\nI do his subject worthy goat's word; and I will not.\\n\\nGLOUCESTER:\\nThis diell die; my master compliy'd by\\nin hour with honour from the sword by any thing.\\n\\nKING RICHARD III:\\nThink not thy late, bid, some another hear\\nOurs, \\n\\n-------------------------------------\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from 3x400, seq len 200\n",
    "\n",
    "'''\n",
    "ROMEO:\n",
    "Must with your victoties are have turn'd.\n",
    "\n",
    "First Servingman:\n",
    "Why, war, now follows a well-maid, well not would a\n",
    "gottour husband,\n",
    "That I can wish ear to behold his worship,\n",
    "Which with the rest, when you wrong and so her,\n",
    "Twenty your presentate: and he will know\n",
    "So minapet is to fall, of this yould by\n",
    "The sting o' the royal unten to hour:\n",
    "But I am so please, and see my lord\n",
    "But at it here was I w\n",
    "\n",
    "-------------------------------------\n",
    "\n",
    "\n",
    "ROMEO:\n",
    "The lastest in all soul of oppot; but what you may:\n",
    "Where is your brother?\n",
    "\n",
    "GREEN:\n",
    "Well me, before I pas there, and why what is la?\n",
    "Should you have been foe in out of thinkes,\n",
    "I must not prose debthe and thing to die:\n",
    "I had to leat way moved him to the same,\n",
    "And so he end in the distreation of yourself.\n",
    "If I am of soversigatio, they do impershind\n",
    "A dead deliner by Bolingbroke.\n",
    "With you but love \n",
    "\n",
    "-------------------------------------\n",
    "\n",
    "\n",
    "ROMEO:\n",
    "But say that Bay, so will I.\n",
    "\n",
    "PERDITA:\n",
    "O\n",
    "Light, there be so grow by were I for a wholeson:\n",
    "Speak against this holy son?\n",
    "\n",
    "Second Murderer:\n",
    "'Zounds, what mitadvance there's men i' the crown?\n",
    "\n",
    "Clown:\n",
    "'Twere eye to deep the membles: my fair laid, he's\n",
    "poacting thy set thee, which we do use here? what, ho!\n",
    "What worke, letsown, you not?\n",
    "\n",
    "COMINIUS:\n",
    "Cuter!\n",
    "\n",
    "MENENIUS:\n",
    "For, marry, so at yother better have\n",
    "\n",
    "-------------------------------------\n",
    "\n",
    "\n",
    "ROMEO:\n",
    "O, for this grievous father,\n",
    "One breath the prince my sorties art a passage\n",
    "Even in my holour, be or and fool about\n",
    "In mine own times, stroke an ant's leaves of the part\n",
    "As with a manners do look now, to be dead.\n",
    "Either thou art, hence. As she's none enemy,\n",
    "Soften her beseasure of the thatn of him\n",
    "When I will know the at in blessing:\n",
    "need we come mean there all unburning thee\n",
    "And do myself and o\n",
    "\n",
    "-------------------------------------\n",
    "\n",
    "\n",
    "ROMEO:\n",
    "The proud mischance with nothing's disposition;\n",
    "And not with any miness noble undertoos:\n",
    "He hath sainting difference them to speak unto their\n",
    "friends, that seek for yourselp; but, his office, will\n",
    "you an ever's roodly gog, as welcome, but we arrive,\n",
    "what couldste to their master, whereing that is cast\n",
    "shear, son; my lord and honour's.\n",
    "\n",
    "KING RICHARD III:\n",
    "Why, so much unfancy with me rather there!\n",
    "\n",
    "-------------------------------------\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "me we shall not like.\n",
    "\n",
    "ESCALUS:\n",
    "And we do proud? Where is he cames.\n",
    "\n",
    "LUCIO:\n",
    "Become by Saint in his boy, being modest:\n",
    "My noble lords have last to lose his own,\n",
    "The love be mew'd in such a dudd of mocker.\n",
    "\n",
    "QUEEN MARGARET:\n",
    "Nurserves upon some paly, look at his sil.\n",
    "\n",
    "EDWARD:\n",
    "Claimio is will do this white fall of death.\n",
    "\n",
    "KING RICHARD III:\n",
    "My lord, he charge thee, tender may in most,\n",
    "Whom I find poor thrive gives to him my love.\n",
    "\n",
    "WARWICK:\n",
    "O maline that fellow, if a begin for me!\n",
    "\n",
    "QUEEN MARGARET:\n",
    "That will not be mine houglips and tell it:\n",
    "Ra' Henry, let us well devilling made\n",
    "A great sise, by the oop\n",
    "\n",
    "-------------------------------------\n",
    "\n",
    "me possess you of things apprehends.\n",
    "\n",
    "WARWICK:\n",
    "How now! what news?\n",
    "\n",
    "Messenger:\n",
    "My gracious lord,\n",
    "That there do his thing that only live.\n",
    "\n",
    "HENRY BOLINGBROKE:\n",
    "You bestrick, and reason'd mensor--\n",
    "Was much with the impostable face!\n",
    "\n",
    "BENVOLIO:\n",
    "Ay, but this buried.\n",
    "\n",
    "PERDITA:\n",
    "You were command.\n",
    "\n",
    "CLIFFORD:\n",
    "Methor; and the rest is raskind; and I\n",
    "shall away thy sirk, sweet belly.\n",
    "\n",
    "ROMEO:\n",
    "I know thy unk to him, we will not\n",
    "I will be vow'd, and 'tis time, thou wilt thou not\n",
    "Escrees though, who I know\n",
    "To light our lobss, and he dishonour'd blood.\n",
    "Is hewn, Amal, More out or losuliess that he be\n",
    "That a wound ou\n",
    "\n",
    "-------------------------------------\n",
    "\n",
    "mel'd many condemned shame:\n",
    "Reproach false which each must be king, with raidings\n",
    "Shall never bonge for the swifter of this worse.\n",
    "\n",
    "BALTHASAR:\n",
    "A brother's traitors, of me, I'll not seen,\n",
    "With us hence spend can with are quive.\n",
    "\n",
    "BENVOLIO:\n",
    "And I love as to be one with recimate\n",
    "And all their world, but tyrants for our angs\n",
    "Shall stop ord should have gives but their\n",
    "And make masquers to the sworch of with a league\n",
    "And with me by any clain 'em: I must be\n",
    "To use my will the people dream, you say:\n",
    "I dreamt and bring him by my occenather.\n",
    "\n",
    "DUKE OF AUMERLE:\n",
    "Even where I bear your shoulds nor we will well\n",
    "\n",
    "-------------------------------------\n",
    "\n",
    "mepher.\n",
    "\n",
    "ISABELLA:\n",
    "True from his face.\n",
    "\n",
    "PAULINA:\n",
    "He's dead, which tost thou will do't,\n",
    "While these touchesment a vain: here's the moon\n",
    "Like the worse of yours?\n",
    "\n",
    "VOLUMNIA:\n",
    "She is have see,\n",
    "The lead that cannot leave.\n",
    "\n",
    "MARIANA:\n",
    "O that I am sure,\n",
    "And he hath not been abuse that things to-day\n",
    "For what the sunsty say to say they from me;\n",
    "And when your sovereign king, will sate thee;\n",
    "Our lords of great commanding and my way;\n",
    "To it, and 'tis unhappy way in arms.\n",
    "\n",
    "GLOUCESTER:\n",
    "This follow my tongue like a love that she know\n",
    "Thou grows thy wither invoctions them.\n",
    "\n",
    "KING EDWARD IV:\n",
    "Ay, nor night belormer ha\n",
    "\n",
    "-------------------------------------\n",
    "\n",
    "me: speak?\n",
    "\n",
    "MENENIUS:\n",
    "You are mine own credively, and now of revenge:\n",
    "I will be heard with strong come basted axe.\n",
    "\n",
    "Second Murdeo;\n",
    "Belike, disconore,--\n",
    "\n",
    "AUTOLYCUS:\n",
    "Even behaps, my lord! I have piece up to all\n",
    "First, not we look'd him: he is become he hope\n",
    "that woe and were the porely change you what\n",
    "they are.\n",
    "\n",
    "MOPSA:\n",
    "O, they are in a most velconceley, hast\n",
    "you, lords? COMhaRD:\n",
    "I do his subject worthy goat's word; and I will not.\n",
    "\n",
    "GLOUCESTER:\n",
    "This diell die; my master compliy'd by\n",
    "in hour with honour from the sword by any thing.\n",
    "\n",
    "KING RICHARD III:\n",
    "Think not thy late, bid, some another hear\n",
    "Ours, \n",
    "\n",
    "-------------------------------------\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
