{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "og-05TduU4p0"
   },
   "source": [
    "# LSTM CharRNN - with different encoding strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfogBoIsU4p6"
   },
   "source": [
    "### Imports and data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "deIiS_8W67vH"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils as utils\n",
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ag_S2ct0ueI",
    "outputId": "ffdc7fb7-50c0-423e-e737-bc822b319e4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 100\n",
      "{'v': 0, 'L': 1, 'w': 2, 'Q': 3, 't': 4, 'O': 5, 'X': 6, 'W': 7, 'q': 8, 'l': 9, 'F': 10, ';': 11, 'x': 12, 'r': 13, '!': 14, 'T': 15, 'Z': 16, 'C': 17, 'm': 18, 'S': 19, '$': 20, 'a': 21, '?': 22, \"'\": 23, 'f': 24, ',': 25, 'E': 26, ' ': 27, 'b': 28, 'U': 29, 'A': 30, '-': 31, 'G': 32, 'h': 33, 'Y': 34, 'P': 35, 'u': 36, 'y': 37, 'M': 38, 'd': 39, 'H': 40, '.': 41, 'e': 42, 'R': 43, 'K': 44, '&': 45, 'g': 46, 'J': 47, ':': 48, '\\n': 49, 'k': 50, 'I': 51, 'o': 52, 's': 53, 'n': 54, 'V': 55, 'i': 56, 'p': 57, 'j': 58, 'N': 59, 'B': 60, 'c': 61, 'z': 62, 'D': 63, '3': 64}\n",
      "1403 4225\n",
      "{'Fi': 0, 'ir': 1, 'rs': 2, 'st': 3, 't ': 4, ' C': 5, 'Ci': 6, 'it': 7, 'ti': 8, 'iz': 9, 'ze': 10, 'en': 11, 'n:': 12, ':\\n': 13, '\\nB': 14, 'Be': 15, 'ef': 16, 'fo': 17, 'or': 18, 're': 19, 'e ': 20, ' w': 21, 'we': 22, ' p': 23, 'pr': 24, 'ro': 25, 'oc': 26, 'ce': 27, 'ee': 28, 'ed': 29, 'd ': 30, ' a': 31, 'an': 32, 'ny': 33, 'y ': 34, ' f': 35, 'fu': 36, 'ur': 37, 'rt': 38, 'th': 39, 'he': 40, 'er': 41, 'r,': 42, ', ': 43, ' h': 44, 'ea': 45, 'ar': 46, 'r ': 47, ' m': 48, 'me': 49, ' s': 50, 'sp': 51, 'pe': 52, 'ak': 53, 'k.': 54, '.\\n': 55, '\\n\\n': 56, '\\nA': 57, 'Al': 58, 'll': 59, 'l:': 60, '\\nS': 61, 'Sp': 62, 'k,': 63, '\\nF': 64, '\\nY': 65, 'Yo': 66, 'ou': 67, 'u ': 68, 'al': 69, 'l ': 70, ' r': 71, 'es': 72, 'so': 73, 'ol': 74, 'lv': 75, 've': 76, 'ra': 77, 'at': 78, ' t': 79, 'to': 80, 'o ': 81, ' d': 82, 'di': 83, 'ie': 84, 'ha': 85, 'n ': 86, 'fa': 87, 'am': 88, 'mi': 89, 'is': 90, 'sh': 91, 'h?': 92, '?\\n': 93, '\\nR': 94, 'Re': 95, 'd.': 96, '. ': 97, 't,': 98, ' y': 99, 'yo': 100, ' k': 101, 'kn': 102, 'no': 103, 'ow': 104, 'w ': 105, 'Ca': 106, 'ai': 107, 'iu': 108, 'us': 109, 's ': 110, ' M': 111, 'Ma': 112, 'rc': 113, 'ci': 114, ' i': 115, ' c': 116, 'ch': 117, 'hi': 118, 'f ': 119, ' e': 120, 'ne': 121, 'em': 122, 'my': 123, 'eo': 124, 'op': 125, 'pl': 126, 'le': 127, 'e.': 128, '\\nW': 129, 'We': 130, \"w'\": 131, \"'t\": 132, 't.': 133, '\\nL': 134, 'Le': 135, 'et': 136, ' u': 137, 'ki': 138, 'il': 139, 'im': 140, 'm,': 141, 'nd': 142, \"e'\": 143, \"'l\": 144, 'av': 145, 'co': 146, 'rn': 147, ' o': 148, 'wn': 149, 'ri': 150, 'ic': 151, '\\nI': 152, 'Is': 153, \"s'\": 154, 'a ': 155, ' v': 156, 'rd': 157, 'ct': 158, 't?': 159, '\\nN': 160, 'No': 161, 'mo': 162, 'ta': 163, 'lk': 164, 'in': 165, 'ng': 166, 'g ': 167, 'on': 168, \"n'\": 169, 't;': 170, '; ': 171, ' l': 172, ' b': 173, 'be': 174, 'do': 175, 'e:': 176, ': ': 177, 'aw': 178, 'wa': 179, 'ay': 180, 'y,': 181, 'y!': 182, '!\\n': 183, 'Se': 184, 'ec': 185, '\\nO': 186, 'On': 187, 'wo': 188, 'd,': 189, ' g': 190, 'go': 191, 'oo': 192, 'od': 193, 'ns': 194, 's.': 195, 'ac': 196, 'cc': 197, 'un': 198, 'nt': 199, 'te': 200, 'po': 201, 's,': 202, 'pa': 203, 'tr': 204, 'ia': 205, 'Wh': 206, 'au': 207, 'ut': 208, 'ho': 209, 'ty': 210, 'su': 211, 'rf': 212, 'fe': 213, 'ei': 214, 'ts': 215, 'ul': 216, 'ld': 217, 'el': 218, 'li': 219, 'ev': 220, 's:': 221, 'if': 222, 'ey': 223, 'y\\n': 224, '\\nw': 225, 'yi': 226, 'bu': 227, 'up': 228, 'fl': 229, 'lu': 230, 'ui': 231, 'wh': 232, 'e\\n': 233, 'om': 234, 'e,': 235, 'ig': 236, 'gh': 237, 'ht': 238, 'gu': 239, 'ue': 240, 'ss': 241, 'hu': 242, 'um': 243, 'ma': 244, 'ly': 245, 'y;': 246, ';\\n': 247, '\\nb': 248, 'nk': 249, 'k ': 250, 'de': 251, 'r:': 252, 'nn': 253, 't\\n': 254, '\\na': 255, 'af': 256, 'ff': 257, 'ob': 258, 'bj': 259, 'je': 260, 'of': 261, 'se': 262, 'ry': 263, 'as': 264, 'n\\n': 265, '\\ni': 266, 'nv': 267, 'cu': 268, 'la': 269, 'ab': 270, 'da': 271, 'nc': 272, 'e;': 273, 'r\\n': 274, '\\ns': 275, 'uf': 276, 'ga': 277, 'm ': 278, ' L': 279, 'ge': 280, 'wi': 281, 'h\\n': 282, '\\no': 283, 'pi': 284, 'ik': 285, 'ke': 286, 'ds': 287, ' I': 288, 'I\\n': 289, 'br': 290, 'ad': 291, ' n': 292, 'ot': 293, 'Wo': 294, 'ag': 295, 's?': 296, 'Ag': 297, 'fi': 298, 't:': 299, \"'s\": 300, 'og': 301, 'mm': 302, 'na': 303, 'lt': 304, 'y.': 305, '\\nC': 306, 'Co': 307, 'si': 308, 'id': 309, 'rv': 310, 'vi': 311, 'y?': 312, '\\nV': 313, 'Ve': 314, 'l;': 315, 'gi': 316, 'iv': 317, 'd\\n': 318, '\\nr': 319, 'ep': 320, 'ys': 321, 'ms': 322, 'lf': 323, 'h ': 324, 'ud': 325, 'Na': 326, 'io': 327, 'sl': 328, 'I ': 329, 'sa': 330, 'u,': 331, 'd:': 332, 'ug': 333, 'ft': 334, 't-': 335, '-c': 336, 'sc': 337, 'ca': 338, '\\nc': 339, 'o\\n': 340, '\\np': 341, 'tl': 342, 'd;': 343, 'tu': 344, 'lp': 345, 'p ': 346, 'a\\n': 347, '\\nv': 348, 'm.': 349, ' Y': 350, 'mu': 351, 'ov': 352, 'If': 353, 'ba': 354, 'rr': 355, 's;': 356, '\\nh': 357, 'rp': 358, 'n.': 359, 'e?': 360, '? ': 361, ' T': 362, 'Th': 363, \"o'\": 364, \"' \": 365, 'hy': 366, 'ap': 367, 'l!': 368, 'So': 369, 't!': 370, '! ': 371, 'Me': 372, 'ni': 373, ' A': 374, 'gr': 375, 'ip': 376, 'pp': 377, 'a;': 378, 'lw': 379, 'lo': 380, '\\nt': 381, '\\nH': 382, 'He': 383, 'h:': 384, 'o!': 385, '\\nM': 386, 'ME': 387, 'EN': 388, 'NE': 389, 'NI': 390, 'IU': 391, 'US': 392, 'S:': 393, 'rk': 394, \"k'\": 395, 'ym': 396, 'n,': 397, 'd?': 398, 'u\\n': 399, 'Wi': 400, 'cl': 401, 'ub': 402, 'bs': 403, 'tt': 404, 'r?': 405, 'u.': 406, 'Ou': 407, 'kl': 408, 'tn': 409, 'o,': 410, ',\\n': 411, \" '\": 412, \"'e\": 413, 'hs': 414, 'rm': 415, 'o.': 416, 'fr': 417, 'hb': 418, 'bo': 419, 'lr': 420, 'dy': 421, 'os': 422, 'bl': 423, 'Ha': 424, ' F': 425, 'Fo': 426, 'h,': 427, 'l\\n': 428, 'St': 429, 'm\\n': 430, ' R': 431, 'Ro': 432, '\\nT': 433, 'cr': 434, 'ck': 435, 'rb': 436, 's\\n': 437, 'Of': 438, 'Ap': 439, 'mp': 440, 'p.': 441, 'by': 442, 'lm': 443, 's!': 444, 'Tr': 445, 'ru': 446, 'd!': 447, '\\ny': 448, 'ye': 449, 'e-': 450, '-h': 451, 'n;': 452, '\\ne': 453, 'r.': 454, 'p,': 455, '\\nE': 456, 'Ei': 457, 'nf': 458, 'dr': 459, 'Or': 460, 'A ': 461, 'Bu': 462, 'pu': 463, 'To': 464, 'l,': 465, \"I'\": 466, '\\nf': 467, 'b ': 468, 'sg': 469, \"y'\": 470, 'mb': 471, 'eb': 472, \"l'\": 473, \"'d\": 474, 'nl': 475, 'dl': 476, 'pb': 477, 'oa': 478, 'g\\n': 479, 'Li': 480, '\\nD': 481, 'Di': 482, 'uc': 483, 'An': 484, 'ua': 485, '\\nU': 486, 'Un': 487, 'sw': 488, \"r'\": 489, 'd-': 490, '--': 491, '-\\n': 492, 'Si': 493, ' W': 494, 'sm': 495, 'gs': 496, 's-': 497, 'ok': 498, 'As': 499, 'k-': 500, '-i': 501, 'gl': 502, 'pt': 503, 'gn': 504, 'y-': 505, 'eg': 506, 'g,': 507, 'ps': 508, 'In': 509, 'c,': 510, 'n?': 511, \"\\n'\": 512, \"'F\": 513, 'ks': 514, 'Sh': 515, ',-': 516, \"u'\": 517, 'l-': 518, '-o': 519, '\\nP': 520, 'Pa': 521, 'Ye': 522, \"'r\": 523, 'ib': 524, \"'T\": 525, \",'\": 526, ' q': 527, 'qu': 528, 'uo': 529, 'p\\n': 530, 'y:': 531, 'hr': 532, 'Ev': 533, 'Fr': 534, 'cy': 535, \"'-\": 536, '-t': 537, 'Ay': 538, 'r;': 539, 'l.': 540, \".'\": 541, 'It': 542, 'ex': 543, 'xa': 544, 'ls': 545, 'c ': 546, 'oe': 547, 'e!': 548, \"d'\": 549, 'va': 550, 'oi': 551, 'MA': 552, 'AR': 553, 'RC': 554, 'CI': 555, \"t'\": 556, 'bb': 557, 'bi': 558, 'tc': 559, 'bh': 560, 'g.': 561, ' H': 562, 'ox': 563, 'xe': 564, 'bd': 565, 'du': 566, ' j': 567, 'ju': 568, 'De': 569, 'Up': 570, 'vo': 571, 'ew': 572, 'ws': 573, 'nu': 574, 'Hi': 575, 'rl': 576, 'ek': 577, 'g?': 578, 'f,': 579, 'm!': 580, 'w\\n': 581, \"i'\": 582, 'nj': 583, '\\ng': 584, 'h!': 585, 'hl': 586, ' B': 587, 'p?': 588, 'n-': 589, \"h'\": 590, 'm?': 591, 'vu': 592, 'lg': 593, 'sd': 594, ' J': 595, 'Ju': 596, 'Br': 597, ' V': 598, \"-'\": 599, \"'S\": 600, 'Sd': 601, 'nr': 602, \"f'\": 603, 'Er': 604, 'rg': 605, '\\nG': 606, 'Go': 607, 'gm': 608, 'Vo': 609, \"a'\": 610, ' S': 611, 'Tu': 612, 'Au': 613, 'vy': 614, 'CO': 615, 'OM': 616, 'MI': 617, 'IN': 618, 'm:': 619, 'At': 620, 'Ti': 621, 'La': 622, 'f?': 623, 'TI': 624, 'IT': 625, 'TU': 626, \"'o\": 627, 'eh': 628, 'O,': 629, '-b': 630, 'w,': 631, 'w:': 632, 'pf': 633, 'w.': 634, 'SI': 635, 'IC': 636, 'Wa': 637, 'BR': 638, 'RU': 639, 'UT': 640, 'eq': 641, '-m': 642, 'Su': 643, 'Fa': 644, 'tm': 645, 'dd': 646, \"'O\": 647, 'O ': 648, \"!'\": 649, \"'\\n\": 650, 'Op': 651, 'Ho': 652, 'Mo': 653, 'AU': 654, 'UF': 655, 'FI': 656, 'ID': 657, 'DI': 658, 'mv': 659, 'k\\n': 660, 'u:': 661, 'bt': 662, \"m'\": 663, 'By': 664, 'Ta': 665, 'i:': 666, \"'v\": 667, ' N': 668, 'rw': 669, 'tw': 670, 'u!': 671, 'VO': 672, 'OL': 673, 'LU': 674, 'UM': 675, 'MN': 676, 'IA': 677, 'A:': 678, 'g;': 679, 'xp': 680, '\\nm': 681, 'mf': 682, 'sb': 683, 'ej': 684, 'jo': 685, 'r-': 686, 'b,': 687, 'az': 688, 'I,': 689, '\\nn': 690, '-l': 691, 'f\\n': 692, '\\nd': 693, 'm;': 694, 'oy': 695, 'VI': 696, 'IR': 697, 'RG': 698, 'GI': 699, 'IL': 700, 'LI': 701, 'oz': 702, 'Ge': 703, 'Va': 704, 'f.': 705, \"'C\": 706, \":'\": 707, 'sk': 708, 'w!': 709, ' O': 710, 'Aw': 711, 'ph': 712, 'a,': 713, ' G': 714, 'Gr': 715, 'mn': 716, 'Te': 717, 'lc': 718, \"'h\": 719, 'VA': 720, 'AL': 721, 'LE': 722, 'ER': 723, 'RI': 724, 'My': 725, 'Sw': 726, '-k': 727, 'h.': 728, 'p;': 729, '\\nl': 730, \"O'\": 731, 'dn': 732, 'gt': 733, 'u?': 734, ' P': 735, 'Pe': 736, 'ya': 737, ' U': 738, 'Ul': 739, 'c\\n': 740, 'xc': 741, 'u;': 742, 'h;': 743, 'i;': 744, 'Gi': 745, 'Pr': 746, 'Vi': 747, 'LA': 748, 'RT': 749, 'Sa': 750, 'w;': 751, 'uy': 752, 'k!': 753, 'Ar': 754, 'Ra': 755, 'p:': 756, 'f!': 757, 'Am': 758, 'Ad': 759, 'dv': 760, 'wr': 761, 'dg': 762, 'f-': 763, '-B': 764, 'Bo': 765, 'Pl': 766, 'Fu': 767, 'r!': 768, 'I.': 769, 'Sl': 770, 'Cl': 771, \"p'\": 772, 'td': 773, 'Lo': 774, 'hm': 775, 'Cu': 776, 'Ir': 777, 'Pi': 778, 'Mi': 779, '-p': 780, 'kf': 781, 'i ': 782, 'Ab': 783, 'dw': 784, 'Fl': 785, 'xi': 786, 'yh': 787, '-a': 788, '!-': 789, 'l?': 790, 'o?': 791, 'g:': 792, 'f;': 793, 'eu': 794, 'Fe': 795, 'Af': 796, 'ix': 797, 'x ': 798, 'Wr': 799, 'gg': 800, \"g'\": 801, 'dm': 802, \"'W\": 803, 'xt': 804, \"'g\": 805, 'Ne': 806, '-f': 807, 'g!': 808, '.-': 809, 'yp': 810, 'i,': 811, 'CA': 812, 'AI': 813, 'S ': 814, 'OR': 815, 'IO': 816, 'AN': 817, 'NU': 818, 'S!': 819, 'o:': 820, 'wb': 821, 'Mu': 822, 'k:': 823, 'i\\n': 824, ' D': 825, 'n!': 826, 'Tw': 827, 'f:': 828, 'Em': 829, 'rq': 830, 'u-': 831, 'o-': 832, '-n': 833, 'b.': 834, 'ae': 835, 'Es': 836, '-w': 837, 'nm': 838, '\\nu': 839, '-I': 840, 'Ly': 841, 'yc': 842, 'aj': 843, 'sy': 844, 'yl': 845, '\\nk': 846, '-s': 847, '-d': 848, 'a!': 849, 'Ga': 850, '\\nK': 851, 'Kn': 852, 'wl': 853, '?-': 854, '-C': 855, 'b-': 856, 'nh': 857, 'kr': 858, \"'b\": 859, 'Do': 860, '-g': 861, 'wd': 862, 'Ph': 863, 'Du': 864, 'p-': 865, 'fs': 866, 'dk': 867, 'Jo': 868, 'sn': 869, 'uk': 870, 'fy': 871, 'k?': 872, 'df': 873, 'zo': 874, 'w-': 875, '-e': 876, 'ax': 877, 'k;': 878, '-q': 879, 'Ru': 880, 'Pu': 881, 'tp': 882, 'o;': 883, 'ku': 884, \";'\": 885, 'hw': 886, \"'I\": 887, '-P': 888, ':-': 889, \"'L\": 890, 'Bi': 891, 'Ki': 892, 'a?': 893, 'iq': 894, 'x\\n': 895, 'En': 896, 'nw': 897, 'Ce': 898, \"'a\": 899, 'ii': 900, 'Ty': 901, 'Nu': 902, ' Q': 903, 'Qu': 904, '  ': 905, ' \\n': 906, 'Sc': 907, 'za': 908, 'Yi': 909, 'w?': 910, 'nq': 911, 'Ea': 912, 'Ch': 913, \"'?\": 914, \"'!\": 915, 'Hy': 916, 'yd': 917, 'nb': 918, \"'w\": 919, ' &': 920, '&C': 921, 'C:': 922, 'AE': 923, 'Ed': 924, \"'P\": 925, \"'B\": 926, 'hd': 927, 'c;': 928, 'c.': 929, 'ka': 930, 'wf': 931, 'm-': 932, 'Ac': 933, 'oq': 934, 'lb': 935, 'Sm': 936, 'p!': 937, \"'m\": 938, 'yr': 939, \"'D\": 940, 'ml': 941, 'Ke': 942, 'Dr': 943, 'bm': 944, \"'G\": 945, \"b'\": 946, '-y': 947, \"x'\": 948, 'a-': 949, 'xs': 950, 'Ba': 951, ' E': 952, 'Ni': 953, 'yf': 954, \"'f\": 955, 'rj': 956, 'h-': 957, 'py': 958, ' K': 959, 'mt': 960, '- ': 961, '\\nq': 962, 'I;': 963, 'xy': 964, 'gy': 965, 'ko': 966, 'Bl': 967, 'Cr': 968, ';-': 969, '\\nJ': 970, 'c-': 971, 'hc': 972, 'sq': 973, 'b\\n': 974, 'Ri': 975, 'cq': 976, 'ao': 977, \"'R\": 978, 'np': 979, 'ah': 980, 'Ja': 981, 'yn': 982, 'tf': 983, 'ln': 984, 'dc': 985, 'Ol': 986, 'cs': 987, \"A'\": 988, 'Da': 989, 'rh': 990, 'hn': 991, 'O!': 992, 'kb': 993, 'oj': 994, \"'n\": 995, 'i?': 996, \"'y\": 997, \"'H\": 998, \"'M\": 999, 'GL': 1000, 'LO': 1001, 'OU': 1002, 'UC': 1003, 'CE': 1004, 'ES': 1005, 'ST': 1006, 'TE': 1007, 'R:': 1008, '-v': 1009, 'g-': 1010, \"G'\": 1011, 'CL': 1012, 'RE': 1013, 'NC': 1014, 'E:': 1015, '-r': 1016, 'G.': 1017, 'G\\n': 1018, 'G,': 1019, 'Hu': 1020, 'RA': 1021, 'AK': 1022, 'KE': 1023, 'NB': 1024, 'BU': 1025, 'UR': 1026, 'RY': 1027, 'Y:': 1028, 'HA': 1029, 'AS': 1030, 'NG': 1031, 'GS': 1032, 'uz': 1033, 'zz': 1034, 'AD': 1035, 'DY': 1036, 'Y ': 1037, 'NN': 1038, 'Po': 1039, 'Av': 1040, 'xh': 1041, 'Im': 1042, 'GE': 1043, 'NT': 1044, 'TL': 1045, 'EM': 1046, 'N:': 1047, '-F': 1048, 'wk': 1049, 'IV': 1050, 'VE': 1051, 'RS': 1052, 'GR': 1053, 'EY': 1054, '\\nQ': 1055, 'QU': 1056, 'UE': 1057, 'EE': 1058, 'N ': 1059, 'EL': 1060, 'IZ': 1061, 'ZA': 1062, 'AB': 1063, 'BE': 1064, 'ET': 1065, 'TH': 1066, 'H:': 1067, 'Oh': 1068, 'Gl': 1069, 'CK': 1070, 'KI': 1071, 'GH': 1072, 'AM': 1073, 'M:': 1074, 'DE': 1075, 'RB': 1076, 'BY': 1077, 'yw': 1078, 'ih': 1079, 'Ai': 1080, \"':\": 1081, 'GA': 1082, 'T:': 1083, 'Je': 1084, 'gd': 1085, 'DO': 1086, 'SE': 1087, 'Ex': 1088, 'b!': 1089, 'Ur': 1090, 'ky': 1091, 'AT': 1092, 'SB': 1093, 'hf': 1094, 'a:': 1095, 'a.': 1096, \"?'\": 1097, \"'j\": 1098, \"'Z\": 1099, 'Zo': 1100, 'Ah': 1101, 'I:': 1102, 'G ': 1103, 'ED': 1104, 'DW': 1105, 'WA': 1106, 'RD': 1107, 'D ': 1108, 'V:': 1109, ' z': 1110, 'Ox': 1111, 'xf': 1112, 'DU': 1113, 'CH': 1114, 'HE': 1115, 'SS': 1116, 'OF': 1117, 'F ': 1118, 'YO': 1119, 'RK': 1120, 'K:': 1121, ' :': 1122, 'Lu': 1123, 'HB': 1124, 'BI': 1125, 'IS': 1126, 'SH': 1127, 'HO': 1128, 'OP': 1129, 'P ': 1130, '-S': 1131, 'wt': 1132, \"'A\": 1133, 'PR': 1134, 'E ': 1135, 'D:': 1136, 'hh': 1137, 'NA': 1138, 'L:': 1139, 'zi': 1140, 'tg': 1141, \"'c\": 1142, 'TA': 1143, 'NL': 1144, 'oh': 1145, 'TC': 1146, 'IF': 1147, 'FF': 1148, 'F:': 1149, 'UG': 1150, 'sf': 1151, 'LY': 1152, 'El': 1153, \"'i\": 1154, 'OV': 1155, 'Gu': 1156, 'dh': 1157, 'ux': 1158, 'xu': 1159, 'ez': 1160, \"'z\": 1161, 'NO': 1162, 'OT': 1163, 'II': 1164, 'TY': 1165, 'YR': 1166, 'RR': 1167, 'I?': 1168, 'iw': 1169, 'Us': 1170, 'Il': 1171, 'HR': 1172, 'TO': 1173, 'PH': 1174, 'HM': 1175, 'MO': 1176, 'ON': 1177, 'ND': 1178, 'mw': 1179, 'OX': 1180, 'XF': 1181, 'FO': 1182, 'BL': 1183, 'UN': 1184, 'SU': 1185, 'RF': 1186, 'LK': 1187, 'Gh': 1188, 'DS': 1189, 'wy': 1190, '-u': 1191, 'JO': 1192, 'OH': 1193, 'HN': 1194, 'NR': 1195, 'BO': 1196, 'GB': 1197, 'RO': 1198, 'OK': 1199, 'OW': 1200, 'WB': 1201, 'AY': 1202, 'gb': 1203, 'Ob': 1204, 'gf': 1205, 'UK': 1206, 'RL': 1207, 'HY': 1208, 'i-': 1209, 'HU': 1210, 'MB': 1211, '-G': 1212, 'OS': 1213, 'WI': 1214, 'LL': 1215, 'BA': 1216, 'AG': 1217, 'GO': 1218, 'PE': 1219, 'CY': 1220, 'wm': 1221, 'EA': 1222, 'L ': 1223, 'SA': 1224, 'SL': 1225, 'R ': 1226, 'EP': 1227, 'SC': 1228, 'CR': 1229, 'OO': 1230, 'P:': 1231, 'Sn': 1232, \"'K\": 1233, 'ja': 1234, 'TZ': 1235, 'ZW': 1236, 'tz': 1237, 'zw': 1238, 'Ov': 1239, \"'J\": 1240, \"'p\": 1241, 'i.': 1242, 'EX': 1243, 'XT': 1244, 'x,': 1245, 'MP': 1246, 'PS': 1247, 'SO': 1248, 'EG': 1249, 'AH': 1250, 'NV': 1251, 'O:': 1252, 'YB': 1253, 'LT': 1254, 'AP': 1255, 'PU': 1256, 'UL': 1257, 'GU': 1258, '-H': 1259, 'yb': 1260, 'wu': 1261, 'EO': 1262, 'z,': 1263, 'z.': 1264, 'xq': 1265, 'PA': 1266, 'kw': 1267, 'JU': 1268, 'IE': 1269, '-N': 1270, 'hq': 1271, \"'Y\": 1272, 'x.': 1273, 'CU': 1274, 'zy': 1275, 'hp': 1276, 'Et': 1277, 'x;': 1278, '-M': 1279, 'dj': 1280, 'Ec': 1281, 'yv': 1282, '-j': 1283, 'FR': 1284, 'b;': 1285, 'i!': 1286, '-L': 1287, '-O': 1288, 'R.': 1289, 'I!': 1290, '-W': 1291, \"c'\": 1292, 'c!': 1293, '-T': 1294, \"';\": 1295, 'aq': 1296, 'O?': 1297, 'Cy': 1298, 'b:': 1299, 'zl': 1300, 'Ut': 1301, 'dp': 1302, 'kt': 1303, 'xo': 1304, 'Ey': 1305, 'RW': 1306, 'WE': 1307, 'TM': 1308, 'XE': 1309, '\\n3': 1310, '3 ': 1311, 'IM': 1312, 'pk': 1313, 'Eu': 1314, \"',\": 1315, 'z ': 1316, 'EW': 1317, ' X': 1318, 'XI': 1319, 'Rh': 1320, '&c': 1321, 'a$': 1322, '$l': 1323, 'pw': 1324, 'pm': 1325, 'Ic': 1326, 'HI': 1327, 'DA': 1328, 'MU': 1329, '\\nj': 1330, 'PO': 1331, 'IX': 1332, 'RM': 1333, \"'V\": 1334, 'wc': 1335, 'yt': 1336, 'x-': 1337, 'Sk': 1338, 'Dw': 1339, 'IG': 1340, '\\n-': 1341, 'A\\n': 1342, 'Eq': 1343, 'YC': 1344, 'FL': 1345, 'ZE': 1346, 'mr': 1347, 'xl': 1348, 'gp': 1349, 'sj': 1350, 'x?': 1351, 'bn': 1352, 'TR': 1353, 'UD': 1354, 'km': 1355, 'LB': 1356, 'W:': 1357, 'Ig': 1358, 'Ow': 1359, 'nz': 1360, 'Py': 1361, 'yg': 1362, 'c:': 1363, 'Ka': 1364, 'pd': 1365, 'BH': 1366, 'RN': 1367, 'sv': 1368, 'zu': 1369, 'sr': 1370, \"'u\": 1371, 'E.': 1372, 'c?': 1373, 'Io': 1374, 'PT': 1375, 'KA': 1376, 'NS': 1377, \"'k\": 1378, 'Xa': 1379, 'z!': 1380, 'b?': 1381, 'Aj': 1382, 'tb': 1383, \"'N\": 1384, 'bw': 1385, 'ji': 1386, 'IP': 1387, \"E'\": 1388, 'RV': 1389, 'G-': 1390, 'O\\n': 1391, 'NZ': 1392, 'EB': 1393, 'rz': 1394, 'SP': 1395, 'fn': 1396, 'Sy': 1397, 'IB': 1398, 'FE': 1399, '-D': 1400, 'dt': 1401, 'DR': 1402}\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "with open('shakespeare.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Create character to index and index to character mappings\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "printable_chars = tuple(set(string.printable))\n",
    "printable_int2char = dict(enumerate(printable_chars))\n",
    "printable_char2int = {ch:ii for ii,ch in printable_int2char.items()}\n",
    "\n",
    "# Encode by character\n",
    "encoded = np.array([char2int[ch] for ch in text])\n",
    "\n",
    "\n",
    "## Encode the character pairs within the text\n",
    "double_encoded = []\n",
    "double_char2int = {}\n",
    "\n",
    "# Grab all of the character pairs\n",
    "for i in range(len(text) - 1):\n",
    "    pair = text[i:i+2]\n",
    "    if pair not in double_char2int:\n",
    "        # Assign the next number to any new pairs\n",
    "        double_char2int[pair] = len(double_char2int)\n",
    "    # Encode the character pair with its numerical representation\n",
    "    double_encoded.append(double_char2int[pair])\n",
    "    #print(pair, double_char2int[pair])\n",
    "\n",
    "double_int2char = {i: ch for ch, i in double_char2int.items()}\n",
    "double_encoded = np.array(double_encoded)\n",
    "pairs = tuple(set(double_char2int.keys()))\n",
    "\n",
    "\n",
    "total_chars = len(chars)\n",
    "total_pairs = len(pairs)\n",
    "total_labels = 0\n",
    "\n",
    "\n",
    "# We can see that there are 65 different characters within the tinyshakespeare dataset,\n",
    "# far less than the total amount of printable characters,\n",
    "print(total_chars, len(printable_chars))\n",
    "print(char2int)\n",
    "# Similarly, we can see that there are far less unique pairs of characters within the text than are possible.\n",
    "# We will ignore pairs not included (like zz) to save on processing (and these pairs likely would be ignored anyways)\n",
    "# Though this also means that we'll have to be careful if we ever ask this network to produce text from a string\n",
    "# that isn't included in this encoding.  We'll handle that in a function later.\n",
    "print(total_pairs, total_chars**2)\n",
    "print(double_char2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uMv8TXGY20GG",
    "outputId": "2c3d37ae-47b9-45ba-c938-4547381bbe64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4225"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just to try it, we can also do one with all possible 2-character combinations of the 65 characters used in the data\n",
    "\n",
    "import itertools\n",
    "\n",
    "max_pairs = [''.join(pair) for pair in itertools.product(chars, repeat=2)]\n",
    "total_max_pairs = len(max_pairs)\n",
    "max_char2int = {pair: index for index, pair in enumerate(max_pairs)}\n",
    "max_int2char = {i: ch for ch, i in max_char2int.items()}\n",
    "max_encoded = []\n",
    "\n",
    "for i in range(len(text) - 1):\n",
    "    pair = text[i:i+2]\n",
    "    max_encoded.append(max_char2int[pair])\n",
    "\n",
    "max_encoded = np.array(max_encoded)\n",
    "total_max_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rz_l00sU4p_"
   },
   "source": [
    "## Network definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rq49p_8DU4qA",
    "outputId": "f75d52ca-329e-479d-9e0a-bdefaaa0cabd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_jnID0Ki1gCx"
   },
   "outputs": [],
   "source": [
    "class CharRNN_normal(torch.nn.Module):\n",
    "    def __init__(self, tokens, n_hidden=500, n_layers=2, batch_size=64, drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "\n",
    "        self.labels = tokens #string.printable\n",
    "        self.printable_chars = printable_chars\n",
    "        self.int2char = dict(enumerate(self.labels))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()} #dict(enumerate(self.printable_chars)).items()\n",
    "        self.output_size = len(self.labels) #len(self.printable_chars)\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(self.output_size, n_hidden, n_layers,\n",
    "                                  dropout=drop_prob, batch_first=True)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(drop_prob)\n",
    "\n",
    "        self.linear = torch.nn.Linear(n_hidden, self.output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # self.lstm's x wants (batch_size, seq_length, total_labels)\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        out = self.dropout(r_output)\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        out = self.linear(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size): #=self.batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        if train_on_gpu:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UUb18o6IU4qB"
   },
   "outputs": [],
   "source": [
    "class CharRNN_doubleEncode(torch.nn.Module):\n",
    "    def __init__(self, tokens, n_hidden=300, n_layers=2, batch_size=64, drop_prob=0.2, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "\n",
    "        self.labels = tokens\n",
    "        self.double_int2char = double_int2char\n",
    "        self.double_char2int = double_char2int\n",
    "        self.output_size = len(self.labels)\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(self.output_size, n_hidden, n_layers,\n",
    "                                  dropout=drop_prob, batch_first=True)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(drop_prob)\n",
    "\n",
    "        self.linear = torch.nn.Linear(n_hidden, self.output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # self.lstm's x wants (batch_size, seq_length, total_pairs)\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        out = self.dropout(r_output)\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        out = self.linear(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size): #=self.batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        if train_on_gpu:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "B_NmoZQr75L6"
   },
   "outputs": [],
   "source": [
    "class CharRNN_maxEncode(torch.nn.Module):\n",
    "    def __init__(self, tokens, n_hidden=300, n_layers=2, batch_size=64, drop_prob=0.2, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "\n",
    "        self.labels = tokens\n",
    "        self.double_int2char = max_int2char\n",
    "        self.double_char2int = max_char2int\n",
    "        self.output_size = len(self.labels)\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(self.output_size, n_hidden, n_layers,\n",
    "                                  dropout=drop_prob, batch_first=True)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(drop_prob)\n",
    "\n",
    "        self.linear = torch.nn.Linear(n_hidden, self.output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # self.lstm's x wants (batch_size, seq_length, total_pairs)\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        out = self.dropout(r_output)\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        out = self.linear(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size): #=self.batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        if train_on_gpu:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wm1-nWIcU4qC"
   },
   "source": [
    "## Encoding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BLHRgh2_U4qC"
   },
   "outputs": [],
   "source": [
    "# Function to one-hot encode the characters\n",
    "def one_hot_encode(arr, n_labels=total_labels):\n",
    "    # arr is shape seq_length, batch_size\n",
    "    arr = arr.transpose(1,0)\n",
    "    oh = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    oh[np.arange(oh.shape[0]), arr.flatten()] = 1\n",
    "    oh = oh.reshape((*arr.shape, n_labels))\n",
    "    return oh.transpose(1,0,2)\n",
    "\n",
    "### Actually, didn't end up using these :( ###\n",
    "# # Function to one-hot encode groups of 2 characters, in attempt to increase contextual range\n",
    "# def one_hot_encode_double(arr, n_labels=total_labels):\n",
    "#     # arr is shape seq_length, batch_size\n",
    "#     arr = arr.transpose(1,0)\n",
    "#     if (arr.shape[1]%2 == 1): arr = np.insert(arr, 0, 5, axis=1) # \" \" is 5, to make the new seq_length even\n",
    "#     new = np.zeros((arr.shape[0], int(arr.shape[1]/2)), dtype=int)\n",
    "#     oh = np.zeros((*(new.shape), n_labels**2), dtype=np.float32) # *(new.shape) here because .shape returns a tuple\n",
    "#     # below only works because arr has been pre-processed to be from min-index to max-index\n",
    "#     for i in range(0, arr.shape[1], 2):\n",
    "#         new[:,int(i/2)] = n_labels*arr[:,i] + arr[:,i+1]\n",
    "#     oh[np.arange(oh.shape[0])[:,None], np.arange(oh.shape[1]), new] = 1\n",
    "#     # oh = oh.reshape((*new.shape, n_labels**2))\n",
    "#     # Ex. oh of n**2 = 9, (batch,seq) [1,2],[3,4],[5,0] would be\n",
    "#     # [[0,1,0,0,0,0,0,0,0], [0,0,1,0,0,0,0,0,0]\n",
    "#     #  [0,0,0,1,0,0,0,0,0], [0,0,0,0,1,0,0,0,0]\n",
    "#     #  [0,0,0,0,0,1,0,0,0], [1,0,0,0,0,0,0,0,0]]\n",
    "\n",
    "#     # returns oh of shape seq_length, batch_size, n_labels**2\n",
    "#     return oh.transpose(1,0,2)\n",
    "\n",
    "# def target_to_double(target_arr, n_labels=total_labels):\n",
    "#     target_arr = target_arr.transpose(1,0)\n",
    "#     if (target_arr.shape[1]%2 == 1): target_arr = np.insert(target_arr, 0, 76, axis=1) # \" \" is 5, to make the new seq_length even\n",
    "#     converted = np.zeros((target_arr.shape[0], int(target_arr.shape[1]/2)), dtype=int)\n",
    "#     for i in range(0, target_arr.shape[1], 2):\n",
    "#         converted[:,int(i/2)] = n_labels*target_arr[:,i] + target_arr[:,i+1]\n",
    "#     return converted.transpose(1,0)\n",
    "\n",
    "# def double_to_char(doubled, n_labels=total_labels):\n",
    "#     doubled = doubled.transpose(1,0)\n",
    "#     reverted = np.zeros((doubled.shape[0], int(doubled.shape[1]*2)), dtype=int)\n",
    "#     for i in range(0, doubled.shape[1]):\n",
    "#         reverted[:, i+1] = doubled[:, i] % n_labels\n",
    "#         reverted[:, i] = (doubled[:, i] - reverted[:, i+1]) / n_labels\n",
    "#     return reverted.transpose(1,0)## Encoding functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xNcqF9JU4qD"
   },
   "source": [
    "## Batching functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "JsogAoRn4nPM"
   },
   "outputs": [],
   "source": [
    "# Get a random sequence of the Shakespeare dataset.\n",
    "def get_random_seq_and_target(arr, seq_length):\n",
    "    start_index = random.randint(0, len(arr) - seq_length - 1)\n",
    "    end_index   = start_index + seq_length + 1 - 1\n",
    "    return arr[start_index:end_index], arr[start_index+1:end_index+1]\n",
    "\n",
    "# Get a random paired sequence of the Shakespeare dataset.\n",
    "# Iterates 2 indexes at a time\n",
    "def pair_get_random_seq_and_target(arr, seq_length):\n",
    "    increment = 2\n",
    "    start_index = random.randint(0, len(arr) - 2*seq_length - increment)\n",
    "    end_index   = start_index + 2*seq_length + increment - increment\n",
    "    return arr[start_index:end_index:2], arr[start_index+increment:end_index+increment:2]\n",
    "### WORTH PAYING ATTENTION TO INCREMENT OF TARGET - +1 vs +2 could make a huge difference here\n",
    "\n",
    "\n",
    "def get_batches(arr, batch_size, seq_length, batches_per_iter, pair_mode = False, n_labels=total_labels):\n",
    "    '''Arguments\n",
    "       ---------\n",
    "       arr: Total char array to make batches from, 1-D\n",
    "       batch_size: the number of sequences per batch\n",
    "       seq_length: number of encoded chars per sequence\n",
    "       batches_per_iter: how many sets for batches per iter/epoch\n",
    "       pair_mode: whether to collect from double_encode or not\n",
    "       n_labels: the total number of possible labels\n",
    "    '''\n",
    "\n",
    "    # We want batch to be seq_length,batch_size (128,64)\n",
    "\n",
    "    batch_size_total = batch_size * seq_length\n",
    "    batch = np.zeros((seq_length, batch_size), dtype=int)\n",
    "    target = np.zeros((seq_length, batch_size), dtype=int)\n",
    "\n",
    "    for b in range(0, batches_per_iter):\n",
    "        # iterate through the array, one random sequence at a time\n",
    "        for n in range(0, batch_size):\n",
    "            if pair_mode:\n",
    "                batch[:,n], target[:,n] = pair_get_random_seq_and_target(arr, seq_length)\n",
    "            else:\n",
    "                batch[:,n], target[:,n] = get_random_seq_and_target(arr, seq_length)\n",
    "        yield torch.from_numpy(one_hot_encode(batch, n_labels=n_labels).transpose(1,0,2)), torch.from_numpy(target.transpose(1,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lf6Un3znU4qE"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "R64yCm3D5jZf"
   },
   "outputs": [],
   "source": [
    "def train_step(net, opt, loss_func, batch_size, input, target):\n",
    "    # Initialize hidden state and gradients.\n",
    "    hidden = net.init_hidden(batch_size)\n",
    "    opt.zero_grad()\n",
    "\n",
    "    # Forward pass.\n",
    "    output, hidden = net(input, hidden)\n",
    "\n",
    "    # Compute loss. Flatten output and target tensors and compute cross-entropy.\n",
    "    loss = loss_func(output.reshape(-1, net.output_size), target.reshape(-1))\n",
    "\n",
    "    # Backward pass and optimization.\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "def train(net, data, pair_mode = False, epochs=10, batch_size=64, seq_length=128, lr=0.001, clip=5, val_frac=0.1, print_every=100):\n",
    "    ''' Training a network\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "\n",
    "        net: CharRNN_normal network\n",
    "        data: text data to train the network\n",
    "        pair_mode: whether to refer to double_encode or not\n",
    "        epochs: Number of epochs.iters to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "\n",
    "    '''\n",
    "    all_losses = []\n",
    "    loss_sum   = 0\n",
    "\n",
    "    net.train()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "\n",
    "    counter = 0\n",
    "    'THIS AFFECTS INPUT SIZE'\n",
    "    n_labels = len(net.labels)\n",
    "    batches_per_iter=1000\n",
    "\n",
    "\n",
    "    start_train = timeit.default_timer()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        for input, target in get_batches(data, batch_size, seq_length, batches_per_iter, pair_mode=pair_mode, n_labels=n_labels):\n",
    "            input, target = input.to(device), target.to(device) # Move to GPU memory.\n",
    "            #print(\"input+target shape:\", input.shape, target.shape)\n",
    "            #input+target shape: torch.Size([128, 64, 65]) torch.Size([128, 64])\n",
    "\n",
    "            loss      = train_step(net, opt, loss_func, batch_size, input, target)   # Calculate the loss.\n",
    "            loss_sum += loss                                  # Accumulate the loss.\n",
    "\n",
    "            counter += 1\n",
    "            # Print the log.\n",
    "            if counter % print_every == print_every - 1:\n",
    "                print('iter:{}/{} loss:{}'.format(counter+1, batches_per_iter, loss_sum / print_every))\n",
    "                #print('generated sequence: {}\\n'.format(eval_step(net)))\n",
    "\n",
    "                # Track the loss.\n",
    "                all_losses.append(loss_sum / print_every)\n",
    "                loss_sum = 0\n",
    "\n",
    "        val_h = net.init_hidden(batch_size)\n",
    "        val_losses = []\n",
    "        net.eval()\n",
    "        for input, target in get_batches(val_data, batch_size, seq_length, batches_per_iter=5, pair_mode=pair_mode, n_labels=n_labels):\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "            if(train_on_gpu):\n",
    "                input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            output, val_h = net(input, val_h)\n",
    "            val_loss = loss_func(output.reshape(-1, net.output_size), target.reshape(-1))\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "        net.train() # reset to train mode after iterationg through validation data\n",
    "        print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "              \"Step: {}...\".format(counter),\n",
    "              \"Loss: {:.4f}...\".format(loss),\n",
    "              \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
    "\n",
    "\n",
    "    end_train = timeit.default_timer()\n",
    "    print (\"Training time elapsed:\", end_train - start_train, \"s\")\n",
    "\n",
    "    return loss_sum, all_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgyxi8ZHU4qF"
   },
   "source": [
    "#### Initialize and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jWDaVojH162C",
    "outputId": "33dc50d2-9f50-4994-8aef-842014942b09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN_doubleEncode(\n",
      "  (lstm): LSTM(1403, 500, batch_first=True, dropout=0.3)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (linear): Linear(in_features=500, out_features=1403, bias=True)\n",
      ")\n",
      "4512903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "### Normal CharRNN ###\n",
    "\n",
    "# n_hidden = 300\n",
    "# n_layers = 3\n",
    "# dropout = 0.5\n",
    "# batch_size = 64\n",
    "# seq_length = 128\n",
    "# n_epochs = 20\n",
    "# learning_rate = 0.001\n",
    "\n",
    "# total_labels = total_chars\n",
    "# double = False\n",
    "\n",
    "# net = CharRNN_normal(chars, n_hidden, n_layers)\n",
    "# print(net)\n",
    "# print(sum(p.numel() for p in net.parameters()))\n",
    "# # train the model\n",
    "# loss_sum, all_losses = train(net, encoded, epochs=n_epochs, batch_size=batch_size,\n",
    "#                              seq_length=seq_length, lr=learning_rate, print_every=500)\n",
    "#\n",
    "# torch.save(net.state_dict(), 'SequenceRNN_params_ -layers h- .pth')\n",
    "# with open('SequenceRNN_lossList_ -layers h- _loss- .pkl', 'wb') as file:\n",
    "#     pickle.dump(all_losses, file)\n",
    "\n",
    "\n",
    "### Double Encoded CharRNN ###\n",
    "\n",
    "n_hidden = 500\n",
    "n_layers = 1\n",
    "dropout = 0.3\n",
    "batch_size = 128\n",
    "seq_length = 64\n",
    "n_epochs = 15\n",
    "learning_rate = 0.001\n",
    "\n",
    "total_labels = total_pairs\n",
    "double = True\n",
    "\n",
    "netDouble = CharRNN_doubleEncode(pairs, n_hidden, n_layers, drop_prob=dropout)\n",
    "print(netDouble)\n",
    "print(sum(p.numel() for p in netDouble.parameters()))\n",
    "# train the model\n",
    "# loss_sum, all_losses = train(netDouble, double_encoded, pair_mode = True, epochs=n_epochs, batch_size=batch_size,\n",
    "#                              seq_length=seq_length, lr=learning_rate, print_every=100)\n",
    "\n",
    "# torch.save(netDouble.state_dict(), 'drive/MyDrive/Colab Notebooks/Word_RNN/DoubleRNN_params_3-layers h-300_iter2_cont.pth')\n",
    "# with open('drive/MyDrive/Colab Notebooks/Word_RNN/DoubleRNN_lossList_3-layers h-300_loss- _iter2_cont.pkl', 'wb') as file:\n",
    "#     pickle.dump(all_losses, file)\n",
    "\n",
    "\n",
    "### Max Double Encoded CharRNN ###\n",
    "\n",
    "# n_hidden = 300\n",
    "# n_layers = 2\n",
    "# dropout = 0.3\n",
    "# batch_size = 64\n",
    "# seq_length = 128\n",
    "# n_epochs = 25\n",
    "# learning_rate = 0.001\n",
    "\n",
    "# total_labels = total_max_pairs\n",
    "# double = True\n",
    "\n",
    "# netMax = CharRNN_maxEncode(max_pairs, n_hidden, n_layers, drop_prob=dropout)\n",
    "# print(netMax)\n",
    "# print(sum(p.numel() for p in netMax.parameters()))\n",
    "# # train the model\n",
    "# # loss_sum, all_losses = train(netMax, max_encoded, pair_mode = True, epochs=n_epochs, batch_size=batch_size,\n",
    "# #                              seq_length=seq_length, lr=learning_rate, print_every=100)\n",
    "\n",
    "# torch.save(netMax.state_dict(), 'drive/MyDrive/Colab Notebooks/Word_RNN/MaxRNN_params_3-layers h-300_iter2.pth')\n",
    "# with open('drive/MyDrive/Colab Notebooks/Word_RNN/MaxRNN_lossList_3-layers h-300_loss- _iter2.pkl', 'wb') as file:\n",
    "#     pickle.dump(all_losses, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75a4v383U4qG"
   },
   "source": [
    "#### Training loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "id": "jeZ-5KXuU4qG",
    "outputId": "baf17917-d720-4c2c-9643-acc7e1aa8065"
   },
   "outputs": [],
   "source": [
    "# plt.xlabel('iters')\n",
    "# plt.ylabel('loss')\n",
    "# plt.plot([loss for loss in all_losses])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "cBAbHFbNU4qH"
   },
   "outputs": [],
   "source": [
    "### Single encode ###\n",
    "## 3 layers, 300 hidden, dropout 0.5, 0.001 lr, 20 epochs\n",
    "#  909s train time, 1.271 train loss, 1.609 val loss\n",
    "## blah\n",
    "#  blah\n",
    "\n",
    "### Double encode ###\n",
    "## 2 layers, 300 hidden, dropout 0.2, 0.001 lr, 5 epochs\n",
    "# 275s train time, 1.433 train lost, 1.694 val loss\n",
    "## 3 layers, 400 hidden, dropout 0.3, 0,002 lr, 25 epochs, seq len 200\n",
    "# 3 layers, 300 hidden, dropout 0.3, seq_len 150\n",
    "# 5755 train time, 2.141 train loss, 3.975 val loss\n",
    "# answers don't make much sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYMnj55VU4qI"
   },
   "source": [
    "## Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CharRNN_doubleEncode:\n\tsize mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([2800, 1403]) from checkpoint, the shape in current model is torch.Size([2000, 1403]).\n\tsize mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([2800, 700]) from checkpoint, the shape in current model is torch.Size([2000, 500]).\n\tsize mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([2800]) from checkpoint, the shape in current model is torch.Size([2000]).\n\tsize mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([2800]) from checkpoint, the shape in current model is torch.Size([2000]).\n\tsize mismatch for linear.weight: copying a param with shape torch.Size([1403, 700]) from checkpoint, the shape in current model is torch.Size([1403, 500]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_33976/14685327.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnetDouble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DoubleRNN_params_1-layers h-500_iter2.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1406\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1407\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1408\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CharRNN_doubleEncode:\n\tsize mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([2800, 1403]) from checkpoint, the shape in current model is torch.Size([2000, 1403]).\n\tsize mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([2800, 700]) from checkpoint, the shape in current model is torch.Size([2000, 500]).\n\tsize mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([2800]) from checkpoint, the shape in current model is torch.Size([2000]).\n\tsize mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([2800]) from checkpoint, the shape in current model is torch.Size([2000]).\n\tsize mismatch for linear.weight: copying a param with shape torch.Size([1403, 700]) from checkpoint, the shape in current model is torch.Size([1403, 500])."
     ]
    }
   ],
   "source": [
    "netDouble.load_state_dict(torch.load('DoubleRNN_params_1-layers h-500_iter2.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9PYIAiyK1--X"
   },
   "outputs": [],
   "source": [
    "# Evaluation step function.\n",
    "def eval_step(net, init_seq='W', predicted_len=100, eval_batch_size=3):\n",
    "    # Enter eval mode\n",
    "    net.eval()\n",
    "\n",
    "    # Initialize the hidden state, input and the predicted sequence.\n",
    "    hidden        = net.init_hidden(eval_batch_size)\n",
    "    encoded_seq = np.array([char2int[ch] for ch in init_seq])\n",
    "    init_input    = torch.from_numpy(one_hot_encode(np.array(([encoded_seq]*eval_batch_size))\n",
    "                                    .reshape(eval_batch_size, len(init_seq)).transpose(1,0)).transpose(1,0,2)).to(device)\n",
    "    predicted_seq = np.array(([ch for ch in init_seq]*eval_batch_size)).reshape(eval_batch_size, len(init_seq))\n",
    "    # predicted_seq.shape = (batch, seq, 0)\n",
    "\n",
    "    # Use initial string to \"build up\" hidden state.\n",
    "    #print(init_input[:,0,:].unsqueeze(1).shape)\n",
    "    for t in range(len(init_seq) - 1):\n",
    "        output, hidden = net(init_input[:,t,:].unsqueeze(1), hidden) # input shape (batch_size, seq_length, n_chars)\n",
    "\n",
    "    # Set current input as the last character of the initial string.\n",
    "    input = init_input[:,-1,:].unsqueeze(1)\n",
    "\n",
    "    # Predict more characters after the initial string.\n",
    "    for t in range(predicted_len):\n",
    "        # Get the current output and hidden state.\n",
    "        output, hidden = net(input, hidden)\n",
    "\n",
    "        # Sample from the output as a multinomial distribution.\n",
    "        predicted_index = tuple(torch.multinomial(output[:, :].exp(), 1)[:].flatten().tolist())\n",
    "        #predicted_index = torch.multinomial(output[:, :].exp(), 1)[:].numpy()\n",
    "\n",
    "        # Add predicted character to the sequence and use it as next input.\n",
    "        predicted_chars  = [int2char[i] for i in predicted_index]\n",
    "        predicted_seq = np.concatenate((predicted_seq, np.expand_dims(predicted_chars, axis=1)), axis=1)\n",
    "\n",
    "\n",
    "        # Use the predicted character to generate the input of next round.\n",
    "        input = torch.from_numpy(one_hot_encode(np.array(predicted_index)\n",
    "                                .reshape(1, eval_batch_size)).transpose(1,0,2)).to(device)\n",
    "\n",
    "    return [''.join(row) for row in predicted_seq]\n",
    "\n",
    "\n",
    "\n",
    "# Pairwise evaluation step function.\n",
    "# with iter=1, it produces staggered language... like double letters\n",
    "def double_eval_step(net, init_seq='Wh', predicted_len=100, eval_batch_size=3):\n",
    "    # Enter eval mode\n",
    "    net.eval()\n",
    "\n",
    "    init_seq = [init_seq[i:i+2] for i in range(0, len(init_seq), 2)]\n",
    "\n",
    "    # Initialize the hidden state, input and the predicted sequence.\n",
    "    hidden        = net.init_hidden(eval_batch_size)\n",
    "    encoded_seq = np.array([double_char2int[ch] for ch in init_seq])\n",
    "    init_input    = torch.from_numpy(one_hot_encode(np.array(([encoded_seq]*eval_batch_size))\n",
    "                                    .reshape(eval_batch_size, len(init_seq)).transpose(1,0), n_labels=len(net.labels)).transpose(1,0,2)).to(device)\n",
    "    predicted_seq = np.array(([ch for ch in init_seq]*eval_batch_size)).reshape(eval_batch_size, len(init_seq))\n",
    "    # predicted_seq.shape = (batch, seq, 0)\n",
    "\n",
    "    # Use initial string to \"build up\" hidden state.\n",
    "    #print(init_input[:,0,:].unsqueeze(1).shape)\n",
    "    for t in range(len(init_seq) - 1):\n",
    "        output, hidden = net(init_input[:,t,:].unsqueeze(1), hidden) # input shape (batch_size, seq_length, n_chars)\n",
    "\n",
    "    # Set current input as the last character of the initial string.\n",
    "    input = init_input[:,-1,:].unsqueeze(1)\n",
    "\n",
    "    counter=0\n",
    "    # Predict more characters after the initial string.\n",
    "    for t in range(predicted_len):\n",
    "        counter += 1\n",
    "        # Get the current output and hidden state.\n",
    "        output, hidden = net(input, hidden)\n",
    "\n",
    "        # Sample from the output as a multinomial distribution.\n",
    "        predicted_index = tuple(torch.multinomial(output[:, :].exp(), 1)[:].flatten().tolist())\n",
    "        #predicted_index = torch.multinomial(output[:, :].exp(), 1)[:].numpy()\n",
    "\n",
    "        # Add predicted character to the sequence and use it as next input.\n",
    "        predicted_chars  = [double_int2char[i] for i in predicted_index]\n",
    "        'v TO AVOID DOUBLING PROBLEM, LIKELY FROM ITERATOR IN THE RANDOM SEQUENCE FUNCTION'\n",
    "        #if counter%2 == 0:\n",
    "        predicted_seq = np.concatenate((predicted_seq, np.expand_dims(predicted_chars, axis=1)), axis=1)\n",
    "\n",
    "\n",
    "        # Use the predicted character to generate the input of next round.\n",
    "        input = torch.from_numpy(one_hot_encode(np.array(predicted_index)\n",
    "                                .reshape(1, eval_batch_size), n_labels=len(net.labels)).transpose(1,0,2)).to(device)\n",
    "\n",
    "    return [''.join(row) for row in predicted_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Q-ott-qU4qK"
   },
   "outputs": [],
   "source": [
    "# ## from double L-3 H-300\n",
    "\n",
    "# ROMEO:\n",
    "# Nay, but mercy in the precious then.\n",
    "\n",
    "# ROMEO:\n",
    "# Ay, if you say you will not have your friends\n",
    "# Where I bring that unseen not. Sir John Harry,\n",
    "# And 'tis hear instantly not thank your troth,\n",
    "# Percaint of lives, beggar garland's gentlemen,\n",
    "# For one that very enemy to bear a side.\n",
    "\n",
    "# HENRY BOLINGBROKE:\n",
    "# 'Tis contrary and sylerith capted for.\n",
    "\n",
    "# EDWARD:\n",
    "# 'Tis not in law, by his tongue common passion.\n",
    "# I take you to him into dispatch the cause;\n",
    "# They'll now but loved our good friendly, lay to commeside\n",
    "# Their long unhave, the grave! I will myself,\n",
    "# Say, thought of her despaladis biving and their\n",
    "# Hateows to what should.\n",
    "\n",
    "# AUTOLYCUS:\n",
    "# You have not perily is not the princes of the oracle' histe, and\n",
    "# loss a trop at me; as every executioner,\n",
    "# I seek the opposern old grave entereat.\n",
    "\n",
    "# DUKE VINCENTIO:\n",
    "# Rome of poor might be admoursed.\n",
    "\n",
    "# First Servant:\n",
    "# Or That, I will be strive, for ever have\n",
    "# Though she got, nor shoulders. Why show'd your eyest,\n",
    "# I'll kiss the humblin to have lose a care?\n",
    "# The sake was authority, sir, say\n",
    "# Than e'er that ambious for the field but\n",
    "# If I say would attend your brithering ruast.\n",
    "\n",
    "# Second Musician:\n",
    "# Why, my grantam?\n",
    "\n",
    "# POLIXENES:\n",
    "# Would I infillt within these double loyalty\n",
    "# We look'd us the conduct betwer, I have die?\n",
    "\n",
    "# VOLUMNIA:\n",
    "# Done, orfeny your life, and I have sent to gaze you.\n",
    "\n",
    "# First Lord:\n",
    "# I can have of itself. The stone offence these\n",
    "# voices oft his life were with a old object\n",
    "# Und on your lamenus of in Irivery,\n",
    "# Made it reasons, who can law scarce come\n",
    "# No more than it was by\n",
    "# A waster men.\n",
    "\n",
    "# CAMILLO:\n",
    "# What hast, I will served too?\n",
    "\n",
    "# AUFIDIUS:\n",
    "# All enough for a day ber, if your false life\n",
    "# Fan our leaves; if I am lost do titus\n",
    "# Till wish'd suffering up for young; but he sets me,\n",
    "# A greats of despair, say any throng for yet\n",
    "# 'Twas you make arms, as it a propore,\n",
    "# Swell I repers your name.\n",
    "\n",
    "# GLOUCESTER:\n",
    "# Forbear marry the of the aidings,\n",
    "# Like a good fany to the earth, mather to the wars;\n",
    "# Who though that I should first my brother doth,\n",
    "# And the redese that spars to him to cotus.\n",
    "\n",
    "# NORTHUMBERLAND\n",
    "\n",
    "# -------------------------------------\n",
    "\n",
    "\n",
    "# ROMEO:\n",
    "# There have wear Name of you!\n",
    "\n",
    "# LUCIO:\n",
    "\n",
    "# ISABELLA:\n",
    "# Marchann, the best march them: he was like the hath deny them;\n",
    "# But all the rudent to officit wherein\n",
    "# And that they vile thanks. What till it is thou need?\n",
    "# I have done be sodder--resigns here well.\n",
    "\n",
    "# GREGORY:\n",
    "# O, learn'd this gone of worship to heavy.\n",
    "\n",
    "# ISABELLA:\n",
    "# Where he is banish'd? thou villake cartant\n",
    "# Of a thang to speak: hears it, did breathing,\n",
    "# And, madam, grand York and dece people as harm!\n",
    "# Or that all the arms of my place?\n",
    "\n",
    "# ABHORSON:\n",
    "# 'Tis not not a town. I ne'er chang commanded of your\n",
    "# bidth departs: 'ze would did patience is just\n",
    "# contey'd by safety, speaks, where? it is born as\n",
    "# canst the city resemblis their.\n",
    "\n",
    "# ISABELLA:\n",
    "# If I in thought our till you as I had it.\n",
    "# I have from what you say, sits some offer.\n",
    "\n",
    "# Otchment:\n",
    "# You will not not, sir, be it perform'd father.\n",
    "\n",
    "# First Servingman:\n",
    "# I would a sun by way must hear it. There will prote it with\n",
    "# these curlent; and therefore distast it, draw: to bring him\n",
    "# the sea: beseech the lady.\n",
    "\n",
    "# POLIXENES:\n",
    "# There\n",
    "# is exposs'd as there.\n",
    "\n",
    "# POLIXENES:\n",
    "# She's nothing;\n",
    "# You bad me me to noble; if my faster.\n",
    "\n",
    "# ANGELO:\n",
    "# You have upon such a sentently.\n",
    "\n",
    "# ANGELO:\n",
    "# Yes; I am nute; my lord; what's the Tower?\n",
    "\n",
    "# MENENIUS:\n",
    "# The lady of the maid'st Mencurarlies:--ay,\n",
    "# More, your would have aided so with it in our heat,--\n",
    "# Perhate this cooges starle, fuspish peads hope;\n",
    "# Which does be hand with the per it: in she from\n",
    "# your true image and prosperous vanom'd yours.\n",
    "\n",
    "# LEONTES:\n",
    "# I have a noble captain.\n",
    "\n",
    "# LEONTES:\n",
    "# It will as sunder,\n",
    "# I were runyly well to the Taugh yee.\n",
    "\n",
    "# BRUTUS:\n",
    "# Call all this there is city, come room myself.\n",
    "# What, wilt you not?\n",
    "\n",
    "# PETRUCHIO:\n",
    "# Verina!'' this! O mistress! Most that.\n",
    "\n",
    "# Shepherd:\n",
    "# O, ask our proud, became to us and all embracemens.\n",
    "\n",
    "# VALERIA:\n",
    "# Ay, to what's that?\n",
    "\n",
    "# Second Servant:\n",
    "# Mare you a people dead; if for a doubt was hence of\n",
    "# so;\n",
    "# Six beceforce, or no to pratle I have, which hand\n",
    "# Thou untie these coming you may content\n",
    "# Unlike him to enough our table must\n",
    "# I came to speak of every noddle.\n",
    "\n",
    "# BRUTUS:\n",
    "\n",
    "# -------------------------------------\n",
    "\n",
    "\n",
    "# ROMEO:\n",
    "# Let's live to be so me; too luties here,\n",
    "# Thou art, with God's man, provost, wheely spits\n",
    "# So throat from dream his brother's life.\n",
    "\n",
    "# LADY GREY:\n",
    "# And forbid remember her pursues;\n",
    "# I love, if I am kings, good English crown\n",
    "# And put the duke, and I have lose the stroke,\n",
    "# That would be cured the charible pardon\n",
    "# Of England's party at Frong so afcounter'd\n",
    "# With renente, and wrought glaftey out.\n",
    "\n",
    "# WARWICK:\n",
    "# O, give you not steep: that love your lovings,\n",
    "# Call preital not of his service of the\n",
    "# duke.\n",
    "\n",
    "# WARWICK:\n",
    "# Learn with his hale I see hear on the utterant,\n",
    "# Tullut a gubsd bedited slubbedom'd,\n",
    "# If tidon with the scorts and good to speak;\n",
    "# No wooth though 'twere to go: my uncle person.\n",
    "\n",
    "# JOHN OF GAUNT:\n",
    "# O, look not shall I know I can my love's soul.\n",
    "\n",
    "# KING RICHARD III:\n",
    "# Regriesome that elcome.\n",
    "\n",
    "# KING HENRY VI:\n",
    "# Uncle to your enemies\n",
    "# Bonold in this dungry and fools, but first\n",
    "# With letters of their hatters; and thy virdain\n",
    "# Whose tempy pity I think that I can imbrance make\n",
    "# A warnerre obscuing steeds, were committed,\n",
    "# To neep her dear man, some sees in their\n",
    "# Two from Lord Angelo 'O, what my love?\n",
    "\n",
    "# LUCIO:\n",
    "# Nay, as not yast, if I should accept me here.\n",
    "\n",
    "# TRANIO:\n",
    "# My liege Verans; bed some private hate,\n",
    "# He fesimns and their was truth and resom\n",
    "# Than vice thee an unsinished.\n",
    "\n",
    "# VOLUMNIA:\n",
    "# Nothing, by the own mecsast man.\n",
    "\n",
    "# GREMIO:\n",
    "# And so your company, sir! he is so young;\n",
    "# If thou shalt be mine arm:\n",
    "# That I might nonelo, perjury, there must not,\n",
    "# And here is made a black disgrace again,\n",
    "# For not a skith on fitness again, althous\n",
    "# Is to proclaim these trerbon my little.\n",
    "\n",
    "# CLARENCE:\n",
    "# Untul vice with me, bites can much but some\n",
    "# nurse never hear the prince, be gone only stands.\n",
    "\n",
    "# GLOUCESTER:\n",
    "# The rebel of his city garden near,\n",
    "# Lest I let me wide a weake a ground\n",
    "# As well in charge in you.\n",
    "\n",
    "# YORK:\n",
    "# Ay, which our spirits and my shame's withers:\n",
    "# As I was much yours; and uncle, therefore alack,\n",
    "# Hold'd it he shall bring with thy issue;\n",
    "# Placelyhood freectune is hers.\n",
    "\n",
    "# CLIFFORD:\n",
    "# To me, and thou say, if thou canst know't\n",
    "# For love; g\n",
    "\n",
    "# -------------------------------------\n",
    "\n",
    "\n",
    "# ROMEO:\n",
    "# Pleasant yourselacte! pardon;\n",
    "# Who shall be before to him? I am the old\n",
    "# kings with G,\n",
    "# This my life, she will not fly.\n",
    "\n",
    "# Second King of Bonding most house,\n",
    "# I pray you, that you can, i' the bawd afrang:\n",
    "# Or by the Margaret was prove from these,\n",
    "# And so, some deceived a body of shake,\n",
    "# More Clarence, fres oth too.\n",
    "\n",
    "# TRANIO:\n",
    "# Thou hast neath against my lady's fearful royal;\n",
    "# Thy devil they cannot destence on death,\n",
    "# And in thy life is strength-bach'd. They have wrong\n",
    "# Like a thing exprimit? yet thou hast haste:\n",
    "# And these he finds us most along to leave,\n",
    "# Imliket then tash my laws.\n",
    "\n",
    "# First Senator:\n",
    "# What says I to my fault to hear my oath?\n",
    "\n",
    "# CAPULET:\n",
    "# What send your brothers sudden but a duke?\n",
    "\n",
    "# Servant:\n",
    "# Where's no wife that the good bound from hence?\n",
    "\n",
    "# NORTHUMBERLAND:\n",
    "# Why, come from Bustard Sain, by chair as scen\n",
    "# The tribunes of the proud, sin bal of with him,\n",
    "# Mark'd in the happy like a speech, promits,\n",
    "# The cursting causes out in old meavens night\n",
    "# And havan but that then changel'd there will not\n",
    "# Return to his life. And what have done?\n",
    "\n",
    "# Nurse:\n",
    "# Ah, on that; well shall be the\n",
    "# right in suffering.\n",
    "\n",
    "# BUSHY:\n",
    "# For put we buy by I.\n",
    "\n",
    "# QUEEN MARGARET:\n",
    "# I am not doly did charmhine sing:\n",
    "# But the thought it before, Cogain! God never!\n",
    "\n",
    "# MONTAGUE:\n",
    "# That's her, the Briston manly lord was lad,\n",
    "# And left all the accusation\n",
    "# To take us, now to make me to the brother:\n",
    "# And as I lived this will want in one,\n",
    "# From only a king, who begg'd my liege,\n",
    "# And let it like a grave our murdering sport--\n",
    "# I'll hence in thy kingdom her blood now.\n",
    "\n",
    "# Third Murderer:\n",
    "# Nends the poor sovereign and thy ancient strong beman\n",
    "# Toge to good and a bear, arm;\n",
    "# And quiln the move, what thou shouldst comed fort?\n",
    "# That hate a treasong fointroo'd deary prove\n",
    "# Boromine of that washs of the high fellow.\n",
    "# Go both o' the messet with him, I give my spider'd\n",
    "# Come to be upon me; I forswell awral!\n",
    "# And where's not with our company? What airy his ston?\n",
    "# Thou happy'st on mine eyes, and or with remair,\n",
    "# And what they cries my father's sears\n",
    "# They fight to sight, and\n",
    "\n",
    "# -------------------------------------\n",
    "\n",
    "\n",
    "# ROMEO:\n",
    "# And o have at that dence.\n",
    "\n",
    "# PETRUCHIO:\n",
    "# Villain, I cannoid your talk be sudden,\n",
    "# Since that we see me with the way\n",
    "# And he to use him how it by Vilena,\n",
    "# But staft upon the passy; doubet you, my lord,\n",
    "# To take tongue out honour.\n",
    "\n",
    "# FLORIZEL:\n",
    "# I would all his me well:\n",
    "# How fares the fair? I'll read?\n",
    "\n",
    "# LADY CAPULET:\n",
    "# Good for sone, unlike, his revenge, good,\n",
    "# Hath wold away be stars.\n",
    "\n",
    "# ANGELO:\n",
    "# Well, let's their cause blow: we\n",
    "# now: but you are men, I made to get thee as\n",
    "# you may be sixture about on to oppeigr\n",
    "# from our friends: who, and he stands already,\n",
    "# Away; and pereless cooting older,\n",
    "# That is by charges and want of a lasting.\n",
    "# Ha! suet deserved to their awful soul.\n",
    "\n",
    "# BENVOLIO:\n",
    "# Then almost been in thy disonger word,\n",
    "# Than on a cunsed ere her royal villain,\n",
    "# Were they with daughters bres our feeling deedness.\n",
    "# Loves and kind most refer them.\n",
    "\n",
    "# JULIET:\n",
    "# What seest we revolt about it? On what is,\n",
    "# By rudely but have but be beasted not it\n",
    "# Like to deny in to shape but her bottle,\n",
    "# Who in resolves my body's rain to be.\n",
    "\n",
    "# Second Comes:\n",
    "# Good Crudinalains, selly lost must confess;\n",
    "# On this suns of no out for this well after.\n",
    "\n",
    "# LUCIO:\n",
    "# More thus, but being always as that the people\n",
    "# Hath so dear of England's king and something\n",
    "# To wake up seem, and cut't not abserve\n",
    "# And bold this falling fall\n",
    "# Mont traathe bed, uncold supplats of ince,\n",
    "# By what appear her kindred for our joy.\n",
    "\n",
    "# RIVERS:\n",
    "# To faster now of his army;\n",
    "# King of Brains, I slew the meaning of his eyes,\n",
    "# Her friends am Edward, who art the fiend of fine?\n",
    "# It is the fearful murdering his kinsman.\n",
    "# Thou wert at freely and be a noble being fair:\n",
    "# And hence, and craft hither To mistress,\n",
    "# Therefore seem or an oath enjoy me as any\n",
    "# Bright Rome, how would the fight 'twixt neables,\n",
    "# So many fits a shame, there tried from me,\n",
    "# And to desire to be untorcuress.\n",
    "\n",
    "# BALTHASAR:\n",
    "# I will become an other fair kinsmen;\n",
    "# As gentle soul will peace us methings roth!\n",
    "# Had am art home?\n",
    "\n",
    "# DUKE VINCENTIO:\n",
    "# A praise her hunger than the\n",
    "# trive of mine to whose death hath ents:\n",
    "# Go you me: let th\n",
    "\n",
    "# -------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9PlFrPfDU4qJ"
   },
   "outputs": [],
   "source": [
    "batches = 5\n",
    "#generated_seqs = eval_step(net, pair_mode = double, init_seq='ROMEO', predicted_len=200, eval_batch_size=batches)\n",
    "generated_seqs = double_eval_step(netMax, init_seq='When', predicted_len=300, eval_batch_size=batches)\n",
    "\n",
    "\n",
    "for i in range(batches):\n",
    "    print(generated_seqs[i])\n",
    "    print(\"\\n-------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9bGGKzaf2Dpb"
   },
   "outputs": [],
   "source": [
    "# def predict(net, char, h=None, top_k=None):\n",
    "#         ''' Given a character, predict the next character.\n",
    "#             Returns the predicted character and the hidden state.\n",
    "#         '''\n",
    "\n",
    "#         # tensor inputs\n",
    "#         x = np.array([[net.char2int[char]]])\n",
    "#         x = one_hot_encode(x, len(net.chars))\n",
    "#         inputs = torch.from_numpy(x)\n",
    "\n",
    "#         if(train_on_gpu):\n",
    "#             inputs = inputs.cuda()\n",
    "\n",
    "#         # detach hidden state from history\n",
    "#         h = tuple([each.data for each in h])\n",
    "#         # get the output of the model\n",
    "#         out, h = net(inputs, h)\n",
    "\n",
    "#         # get the character probabilities\n",
    "#         p = F.softmax(out, dim=1).data\n",
    "#         if(train_on_gpu):\n",
    "#             p = p.cpu() # move to cpu\n",
    "\n",
    "#         # get top characters\n",
    "#         if top_k is None:\n",
    "#             top_ch = np.arange(len(net.chars))\n",
    "#         else:\n",
    "#             p, top_ch = p.topk(top_k)\n",
    "#             top_ch = top_ch.numpy().squeeze()\n",
    "\n",
    "#         # select the likely next character with some element of randomness\n",
    "#         p = p.numpy().squeeze()\n",
    "#         char = np.random.choice(top_ch, p=p/p.sum())\n",
    "\n",
    "#         # return the encoded value of the predicted char and the hidden state\n",
    "#         return net.int2char[char], h\n",
    "\n",
    "# def sample(net, size, prime='The', top_k=None):\n",
    "\n",
    "#     if(train_on_gpu):\n",
    "#         net.cuda()\n",
    "#     else:\n",
    "#         net.cpu()\n",
    "\n",
    "#     net.eval() # eval mode\n",
    "\n",
    "#     # First off, run through the prime characters\n",
    "#     chars = [ch for ch in prime]\n",
    "#     h = net.init_hidden(1)\n",
    "#     for ch in prime:\n",
    "#         char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "#     chars.append(char)\n",
    "\n",
    "#     # Now pass in the previous character and get a new one\n",
    "#     for ii in range(size):\n",
    "#         char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "#         chars.append(char)\n",
    "\n",
    "#     return ''.join(chars)\n",
    "\n",
    "# print(sample(net, 1000, prime='JULIET', top_k=5))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
